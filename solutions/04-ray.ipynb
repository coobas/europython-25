{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3497647e",
   "metadata": {},
   "source": [
    "# Parallel computation with Ray"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54735612",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/coobas/europython-25/blob/main/04-ray.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0bf1e03",
   "metadata": {},
   "source": [
    "[Ray](https://docs.ray.io/en/latest/index.html) is a set of libraries that (among others) allow an easy parallelisation of Python tasks - both locally and also in clusters. This part is called **Ray Core**.\n",
    "\n",
    "Apart from this, it also provides specialised libraries for data processing (**Ray Data**), for machine learning and even reinforcement learning (**Ray Train**, **Ray Train**, ...). We will not deal with those in this workshop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8631bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this in Google Collab, not needed if you install this package locally\n",
    "!pip install ray[default]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8faadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obligatory imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ray\n",
    "import plotly.express as px\n",
    "from IPython import get_ipython\n",
    "\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce591e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_colab = \"google.colab\" in str(get_ipython())\n",
    "\n",
    "# Download the data which are part of this repo\n",
    "if is_colab:\n",
    "    import urllib\n",
    "    url = \"https://github.com/coobas/europython-25/raw/refs/heads/main/data.parquet\"\n",
    "    urllib.request.urlretrieve(url, \"data.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca58de68",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e000c854",
   "metadata": {},
   "outputs": [],
   "source": [
    "def long_running(i: int) -> int:\n",
    "    \"\"\"A long running task that we will parallelise.\"\"\"\n",
    "    import time\n",
    "    time.sleep(1)\n",
    "    return i * i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2c7a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "long_running(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc67e736",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "[long_running(i) for i in range(10)]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49eab1e0",
   "metadata": {},
   "source": [
    "If these tasks are independent, we can run them in parallel. There are of course options in Python itself:\n",
    "\n",
    "- multiprocessing (https://docs.python.org/3.13/library/multiprocessing.html) \n",
    "- threading (with GIL-releasing code or with caution in free-threaded Python 3.13+)\n",
    "\n",
    "This does scale exactly well if there are more tasks than CPUs / GPUs on your machine...\n",
    "\n",
    "Other options:\n",
    "- [celery](https://docs.celeryq.dev/en/stable/)\n",
    "- [dask](https://docs.dask.org/en/stable/index.html)\n",
    "\n",
    "With their strengths and weaknesses. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a5e9c9",
   "metadata": {},
   "source": [
    "## Use ray"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6e34b8",
   "metadata": {},
   "source": [
    "Ray always runs a server (even implicitly) and executes the task in nodes that it manages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11fc3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07305b3",
   "metadata": {},
   "source": [
    "This either connects to an existing local server, or creates a new one. (In the same way, we can connect to a different one)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00e82ff",
   "metadata": {},
   "source": [
    "### Remote functions\n",
    "\n",
    "Any function we decorate with the `ray.remote` decorator, becomes a **task** that can be submitted to this server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55769580",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def long_running_ray(i: int) -> int:\n",
    "    import time\n",
    "    time.sleep(1)\n",
    "    return i * i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8dff515",
   "metadata": {},
   "outputs": [],
   "source": [
    "long_running_ray(1)  # This will fail"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db49b322",
   "metadata": {},
   "source": [
    "Well, the error message is right. We should use `.remote` (a different one!)\n",
    "\n",
    "Better (and see that we pass arguments the same way):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2e9046",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "long_running_ray.remote(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ebce590",
   "metadata": {},
   "source": [
    "What happened? The task was submitted to the cluster. But asynchronously. We have to capture the **future** object..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f784399",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_id = long_running_ray.remote(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4fcf3d0",
   "metadata": {},
   "source": [
    "...and get its value (synchronously):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecad1b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.get(task_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d22b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "task_ids = [long_running_ray.remote(i) for i in range(10)]\n",
    "ray.get(task_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a324db",
   "metadata": {},
   "source": [
    "## Exercise: Compute k nearest neighbours remotely"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f62167",
   "metadata": {},
   "source": [
    "We will reuse our definitions of kNN functions (slightly modified) and for random points fÃ­nd their neighbours:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b321460",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_K = 4   # How many nearest neighbors to consider\n",
    "\n",
    "def calculate_distances(query_points: np.ndarray, reference_points: np.ndarray, *, n_dim: int = 3) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Calculate mutual Euclidean distances between M query and N reference points.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    query_points: np.ndarray\n",
    "        (M, n_dim+) array of query points\n",
    "    reference_points: np.ndarray\n",
    "        (N, n_dim+) array of reference points\n",
    "    n_dim: int\n",
    "        Number of dimensions to consider (default: 3, for x, y, floor)\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    distances: np.ndarray\n",
    "        (M, N) array of the distances\n",
    "    \"\"\"\n",
    "    # Expand for broadcasting\n",
    "    query_points = query_points[:, np.newaxis,:n_dim]\n",
    "    reference_points = reference_points[np.newaxis, :, :n_dim]\n",
    "    return np.sqrt(np.sum((reference_points - query_points) ** 2, axis=-1))\n",
    "\n",
    "\n",
    "def knn_search(\n",
    "    query_points: np.ndarray,\n",
    "    reference_points: np.ndarray,\n",
    "    k: int,\n",
    "):\n",
    "    \"\"\"\n",
    "    Find k nearest neighbour reference point indices for N query points.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    indices: np.ndarray\n",
    "        (N, k) matrix of integral indices\n",
    "    \"\"\"\n",
    "    distances = calculate_distances(query_points, reference_points).T\n",
    "    return np.argpartition(distances, k, axis=0)[:k].T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc5527e",
   "metadata": {},
   "source": [
    "Let's create some points to test this on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc2af31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_random_points(\n",
    "    n_points: int, *, n_dim: int = 3, seed: int = 42\n",
    ") -> np.ndarray:\n",
    "    # TODO: Fix floor!\n",
    "    np.random.seed(seed)\n",
    "    return np.random.sample((n_points, n_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9382f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_points = create_random_points(5, seed=42)\n",
    "reference_points = create_random_points(10, seed=84)\n",
    "\n",
    "query_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9beca39",
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_distances(query_points, reference_points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce7aae9",
   "metadata": {},
   "source": [
    "First, we run this without ray (for sizable arrays):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d8086c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "query_points = create_random_points(32768, seed=42)\n",
    "reference_points = create_random_points(1000, seed=84)\n",
    "knn_search(query_points, reference_points, k=DEFAULT_K)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea75ba2",
   "metadata": {},
   "source": [
    "**Task**\n",
    "\n",
    "1. Change the knn_search function into a task.\n",
    "2. Submit the computation to ray and compare the results.\n",
    "3. Look at the execution times (and compare to local run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dec6640",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def knn_search_ray(\n",
    "    query_points: np.ndarray,\n",
    "    reference_points: np.ndarray,\n",
    "    k: int,\n",
    "):\n",
    "    \"\"\"\n",
    "    Find k nearest neighbour reference point indices for N query points.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    indices: np.ndarray\n",
    "        (N, k) matrix of integral indices\n",
    "    \"\"\"\n",
    "    distances = calculate_distances(query_points, reference_points).T\n",
    "    return np.argpartition(distances, k, axis=0)[:k].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29fdb21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "knn_id = knn_search_ray.remote(query_points, reference_points, k=DEFAULT_K)\n",
    "knn_results = ray.get(knn_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e276678",
   "metadata": {},
   "source": [
    "**Question:** Is there any improvement yet?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773e9ce7",
   "metadata": {},
   "source": [
    "## Monitoring ray"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18928aa6",
   "metadata": {},
   "source": [
    "Ray comes with a nice dashboard that allows you to observe running jobs. It runs in a local web server, mostly likely http://localhost:8265. This address is not accessible when running within Google Colab, and so you have to use a special trick to show a mini-window forwarded to the dashboard running in the cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3e252d",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_colab = \"google.colab\" in str(get_ipython())\n",
    "\n",
    "if is_colab:\n",
    "    from google.colab import output\n",
    "    output.serve_kernel_port_as_iframe(8265)  # The port may differ!\n",
    "else:\n",
    "    print(\"Not in google Colab. Try the local link, it should work.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff7e661",
   "metadata": {},
   "source": [
    "Now we submit something really huge:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6647a91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_points = create_random_points(10000000, seed=42)\n",
    "reference_points = create_random_points(100, seed=84)\n",
    "task_id = knn_search_ray.remote(query_points, reference_points, k=DEFAULT_K)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ae985f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ray.get(task_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9589eeba",
   "metadata": {},
   "source": [
    "## Exercise: Parallelize kNN execution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4534494f",
   "metadata": {},
   "source": [
    "Ray by itself does not do any optimisations or segments the tasks you send it. So you need to care about splitting the task yourself. Sometimes this is easy, sometimes it requires a deeper thought (or a library ;-)).\n",
    "\n",
    "If you want to improve the computation of nearest neighbours for a bunch of **independent** points, this is really easy (\"embarassingly parallel problem). Just take the 1000000 query points and split them into **batches**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de206ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_batches(array: np.ndarray, max_size: int) -> list[np.ndarray]:\n",
    "    \"\"\"\n",
    "    Split an array into smaller batches of a maximum size.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    array: np.ndarray\n",
    "        The array to split.\n",
    "    max_size: int\n",
    "        The maximum size of each batch.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    list[np.ndarray]\n",
    "        A list of arrays, each with a maximum size of `max_size`.\n",
    "    \"\"\"\n",
    "    return [array[i:i + max_size] for i in range(0, len(array), max_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28665191",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_into_batches(np.arange(10), 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d86a23c6",
   "metadata": {},
   "source": [
    "**Task**: \n",
    "1. Split the query points accordingly (you may experiment with batch_size, 1000 is pretty ok)\n",
    "2. Submit and retrieve the results for the batches\n",
    "3. Combine the results back into one array ([np.vstack](https://numpy.org/doc/stable/reference/generated/numpy.vstack.html) is your friend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983c0154",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_points = create_random_points(10000000, seed=42)\n",
    "reference_points = create_random_points(100, seed=84)\n",
    "\n",
    "batches = split_into_batches(query_points, 1000)\n",
    "batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b321737",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "task_ids = [\n",
    "    knn_search_ray.remote(batch, reference_points, k=DEFAULT_K) for batch in batches\n",
    "]\n",
    "knn_results = ray.get(task_ids)\n",
    "knn_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f671772c",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.vstack(knn_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0e5fc6",
   "metadata": {},
   "source": [
    "**Question** Can we parallelise in the reference points dimension?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8a7388",
   "metadata": {},
   "source": [
    "## Example: Predicting the housing prices with a database of reference points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d53937",
   "metadata": {},
   "source": [
    "We can use the distances to predict the housing prices (in arbitrary units, perhaps the monthly rent in $ for a single-bedroom apartment?) based on our custom regression implementation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b4561b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_prices(query_points, reference_points, k: int = DEFAULT_K):\n",
    "    \"\"\"\n",
    "    Find prices for N data_points.\n",
    "\n",
    "    Parameters:\n",
    "    query_points: np.ndarray\n",
    "        (N, 3) array of query points\n",
    "    reference_points: np.ndarray\n",
    "        (M, 4) array of data points with x, y, floor, and price\n",
    "    k: int\n",
    "        Number of nearest neighbors to consider\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    prices: np.ndarray\n",
    "        (N,) array of prices\n",
    "    \"\"\"\n",
    "    indices = knn_search(query_points, reference_points, k)\n",
    "    prices: np.ndarray = reference_points[indices, 3]\n",
    "    return prices.mean(axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767f7100",
   "metadata": {},
   "source": [
    "We have predefined reference points in an external file, modeled using an (unknown?) analytical function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2394045",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_colab = \"google.colab\" in str(get_ipython())\n",
    "\n",
    "# Download the data which are part of this repo\n",
    "if is_colab:\n",
    "    import urllib\n",
    "    url = \"https://github.com/coobas/europython-25/raw/refs/heads/main/data.parquet\"\n",
    "    urllib.request.urlretrieve(url, \"data.parquet\")\n",
    "    print(\"Data downloaded to data.parquet.\")\n",
    "else:\n",
    "    print(\"Not in Google Colab, skipping data download.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688d1d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_reference_points_df(path: Path = Path(\"../data.parquet\")) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Load reference data points from a parquet file.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    data_points: np.ndarray\n",
    "        (N, 4) array of data points with x, y, floor, and price columns\n",
    "    \"\"\"\n",
    "\n",
    "    return pd.read_parquet(path)\n",
    "    # return df[[\"x\", \"y\", \"floor\", \"price\"]].to_numpy().astype(float)\n",
    "\n",
    "reference_points_df = load_reference_points_df()\n",
    "reference_points_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb88b9e0",
   "metadata": {},
   "source": [
    "And we also want to run something against this. Let's start with random points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05185963",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_random_3d_points(\n",
    "        n_points: int = 100, *, ranges: list[tuple[float, float]] = [(-10, 10), (-10, 10), (1, 20)], ) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Create random 3D points within specified ranges.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    n_points: int\n",
    "        Number of points to generate.    \n",
    "    ranges: list[tuple[float, float]]\n",
    "        List of tuples specifying the range for each dimension.\n",
    "\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    points: np.ndarray\n",
    "        (n_points, len(ranges)) array of random points.\n",
    "    \"\"\"\n",
    "    if not ranges or len(ranges) != 3:\n",
    "        raise ValueError(\"Ranges must be a list of three tuples for x, y, and floor dimensions.\")\n",
    "    df = np.random.uniform(\n",
    "        low=[r[0] for r in ranges],\n",
    "        high=[r[1] for r in ranges],\n",
    "        size=(n_points, len(ranges))\n",
    "    )\n",
    "    df[:, 2] = df[:, 2].astype(int)  # Ensure the floor is an integer\n",
    "    return df\n",
    "\n",
    "query_points = create_random_3d_points()\n",
    "query_points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd4572d",
   "metadata": {},
   "source": [
    "Our query points need to live in the same dimension (i.e. x,y between -10 and 10, floor between 1 and 20). Note that the floor distibution probably is not \"uniform\", but for demonstration purposes, it should not matter that much."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9405e809",
   "metadata": {},
   "source": [
    "Let's see how the whole thing looks without ray:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b5b7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "prices = compute_prices(query_points, reference_points_df.to_numpy(), k=DEFAULT_K)\n",
    "prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291c06af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_points_and_prices(\n",
    "    query_points: np.ndarray, prices: np.ndarray\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Prepare human-friendly output from numpy arrays.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    query_points: np.ndarray\n",
    "        (N, 3) array of query points\n",
    "    prices: np.ndarray\n",
    "        (N,) array of prices\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    df: pd.DataFrame\n",
    "        DataFrame with columns x, y, floor, price\n",
    "    \"\"\"\n",
    "    return pd.DataFrame(\n",
    "        {\n",
    "            \"x\": query_points[:,0],\n",
    "            \"y\": query_points[:,1],\n",
    "            \"floor\": query_points[:,2].astype(int),\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08da8537",
   "metadata": {},
   "outputs": [],
   "source": [
    "combine_points_and_prices(query_points, prices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b25c8d",
   "metadata": {},
   "source": [
    "We already know how to parallelise this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70520155",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note we can apply the decorator directly to the function\n",
    "compute_prices_ray = ray.remote(compute_prices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ca9f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "query_points = create_random_3d_points(100_000)\n",
    "batches = split_into_batches(query_points, 1_000)\n",
    "task_ids = [\n",
    "    compute_prices_ray.remote(batch, reference_points_df.to_numpy(), k=DEFAULT_K) \n",
    "    for batch in batches\n",
    "]\n",
    "np.hstack(ray.get(task_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24025360",
   "metadata": {},
   "source": [
    "This is relatively fine but mind that we are passing the same reference points over and over again to ray (which requires repeated serialisation, ...). It does not matter that much in our case but if the reference points dataframe were larger, we might see a significant performance penalty."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a1f9e9",
   "metadata": {},
   "source": [
    "### Storing objects in ray"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954cf046",
   "metadata": {},
   "source": [
    "What we can do instead, is to pass the object to ray just once and pass its **object id**. Ray automatically cares about using the referenced value.\n",
    "\n",
    "Let's take it to the extreme and compute the price for each of 10,000 points in a separate task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4095cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "query_points = create_random_3d_points(10_000)\n",
    "batches = split_into_batches(query_points, 1_000)\n",
    "task_ids = [\n",
    "    compute_prices_ray.remote(batch, reference_points_df.to_numpy(), k=DEFAULT_K) \n",
    "    for batch in batches\n",
    "]\n",
    "ray.get(task_ids);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e2aaf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "query_points = create_random_3d_points(10_000)\n",
    "batches = split_into_batches(query_points, 1)\n",
    "task_ids = [\n",
    "    compute_prices_ray.remote(batch, reference_points_df.to_numpy(), k=DEFAULT_K) \n",
    "    for batch in batches\n",
    "]\n",
    "ray.get(task_ids);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cddff209",
   "metadata": {},
   "source": [
    "Do you see the degraded performance?\n",
    "\n",
    "Of course it is mostly due to creating a task per point but part of the penalty is coming from passing the array so many times."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f861ca",
   "metadata": {},
   "source": [
    "Putting objects into a ray cluster is very simple, actually:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a936976e",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 9\n",
    "arg_id = ray.put(i)\n",
    "arg_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f3553a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.get(long_running_ray.remote(arg_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a44f6de",
   "metadata": {},
   "source": [
    "**Task**\n",
    "\n",
    "1. Modify the `compute_prices_ray` from above so that `reference_points_df.to_numpy()` is stored in the ray cluster and reused in each task.\n",
    "2. Compare the execution time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b3524c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "query_points = create_random_3d_points(10000)\n",
    "batches = split_into_batches(query_points, 1)\n",
    "reference_id = ray.put(reference_points_df.to_numpy())\n",
    "task_ids = [\n",
    "    compute_prices_ray.remote(batch, reference_id, k=DEFAULT_K) \n",
    "    for batch in batches\n",
    "]\n",
    "ray.get(task_ids);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da39ad63",
   "metadata": {},
   "source": [
    "Is it better now?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872b6700",
   "metadata": {},
   "source": [
    "**Question**: Would putting query points (either whole or batched) to ray help as well?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc8044f",
   "metadata": {},
   "source": [
    "## TODO: Optional: Actors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d189a6d8",
   "metadata": {},
   "source": [
    "We would like to create a map of prices, i.e. sample data in a grid over the allowed area and use our kNN model to predict a price for each of those (given the floor as a parameter). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca859ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_grid(n_points: int = N_POINTS) -> tuple[np.ndarray, ...]:\n",
    "    \"\"\"\n",
    "    Create a homogenous grid of points to create a map.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    x: np.ndarray\n",
    "        Flattened (n_points x n_points,) array of x values\n",
    "    y: np.ndarray\n",
    "        Flattened (n_points x n_points,) array of x values\n",
    "    \"\"\"\n",
    "    # Note: Tested indirectly via `create_query_points`\n",
    "    # TODO: Add floor\n",
    "    x = np.linspace(-LIMIT, LIMIT, n_points)\n",
    "    y = np.linspace(-LIMIT, LIMIT, n_points)\n",
    "    return tuple(arr.flatten() for arr in np.meshgrid(x, y))\n",
    "\n",
    "\n",
    "def create_query_points(n_points: int = N_POINTS, floor: int = 1) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Create a homogenous grid of points with a floor to create a map.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    query_points: np.ndarray\n",
    "        (n_points x n_points, 3) array of query points\n",
    "    \"\"\"\n",
    "    x, y = create_grid(n_points=n_points)\n",
    "    return np.vstack([x, y, np.ones(x.shape[0]) * floor]).T\n",
    "\n",
    "\n",
    "create_query_points(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9309eb",
   "metadata": {},
   "source": [
    "TODO: Create an actor?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3607f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_points(points: np.ndarray) -> None:\n",
    "    \"\"\"\n",
    "    Draw points on a map.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    points: np.ndarray\n",
    "        (N, 3) array of points to draw\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame({\"x\": points[:,0], \"y\": points[:,1]})\n",
    "    fig = px.scatter(df, x=\"x\", y=\"y\", title=\"Query points\")\n",
    "    fig.show()\n",
    "\n",
    "draw_points(create_query_points(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1511b6",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "accelerating-scientific-python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
