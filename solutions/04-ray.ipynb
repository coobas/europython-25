{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3497647e",
   "metadata": {},
   "source": [
    "# Parallel computation with Ray"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54735612",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/coobas/europython-25/blob/main/04-ray.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0bf1e03",
   "metadata": {},
   "source": [
    "[Ray](https://docs.ray.io/en/latest/index.html) is a set of libraries that (among others) allow an easy parallelisation of Python tasks - both locally and also in clusters. This part is called **Ray Core**.\n",
    "\n",
    "Apart from this, it also provides specialised libraries for data processing (**Ray Data**), for machine learning and even reinforcement learning (**Ray Train**, **Ray Train**, ...). We will not deal with those in this workshop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8631bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this in Google Collab, not needed if you install this package locally\n",
    "!pip install ray[default]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8faadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obligatory imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ray\n",
    "import plotly.express as px\n",
    "from IPython import get_ipython\n",
    "\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce591e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_colab = \"google.colab\" in str(get_ipython())\n",
    "\n",
    "# Download the data which are part of this repo\n",
    "if is_colab:\n",
    "    import urllib\n",
    "    url = \"https://github.com/coobas/europython-25/raw/refs/heads/main/data.parquet\"\n",
    "    urllib.request.urlretrieve(url, \"data.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca58de68",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e000c854",
   "metadata": {},
   "outputs": [],
   "source": [
    "def long_running(i: int) -> int:\n",
    "    \"\"\"A long running task that we will parallelise.\"\"\"\n",
    "    import time\n",
    "    time.sleep(1)\n",
    "    return i * i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2c7a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "long_running(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc67e736",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "[long_running(i) for i in range(10)]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49eab1e0",
   "metadata": {},
   "source": [
    "If these tasks are independent, we can run them in parallel. There are of course options in Python itself:\n",
    "\n",
    "- multiprocessing (https://docs.python.org/3.13/library/multiprocessing.html) \n",
    "- threading (with GIL-releasing code or with caution in free-threaded Python 3.13+)\n",
    "\n",
    "This does scale exactly well if there are more tasks than CPUs / GPUs on your machine...\n",
    "\n",
    "Other options:\n",
    "- [celery](https://docs.celeryq.dev/en/stable/)\n",
    "- [dask](https://docs.dask.org/en/stable/index.html)\n",
    "\n",
    "With their strengths and weaknesses. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a5e9c9",
   "metadata": {},
   "source": [
    "## Use ray"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6e34b8",
   "metadata": {},
   "source": [
    "Ray always runs a server (even implicitly) and executes the task in nodes that it manages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11fc3dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(raylet)\u001b[0m Spilled 3052 MiB, 9993 objects, write throughput 809 MiB/s. Set RAY_verbose_spill_logs=0 to disable this message.\n",
      "\u001b[36m(raylet)\u001b[0m Spilled 6105 MiB, 19988 objects, write throughput 1243 MiB/s.\n",
      "\u001b[36m(compute_prices_jax_ray pid=119973)\u001b[0m 2025-07-14 14:33:27.954899: E external/xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "\u001b[36m(raylet)\u001b[0m Spilled 9156 MiB, 29979 objects, write throughput 1186 MiB/s.\n"
     ]
    }
   ],
   "source": [
    "ray.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07305b3",
   "metadata": {},
   "source": [
    "This either connects to an existing local server, or creates a new one. (In the same way, we can connect to a different one)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00e82ff",
   "metadata": {},
   "source": [
    "### Remote functions\n",
    "\n",
    "Any function we decorate with the `ray.remote` decorator, becomes a **task** that can be submitted to this server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55769580",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def long_running_ray(i: int) -> int:\n",
    "    import time\n",
    "    time.sleep(1)\n",
    "    return i * i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8dff515",
   "metadata": {},
   "outputs": [],
   "source": [
    "long_running_ray(1)  # This will fail"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db49b322",
   "metadata": {},
   "source": [
    "Well, the error message is right. We should use `.remote` (a different one!)\n",
    "\n",
    "Better (and see that we pass arguments the same way):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2e9046",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "long_running_ray.remote(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ebce590",
   "metadata": {},
   "source": [
    "What happened? The task was submitted to the cluster. But asynchronously. We have to capture the **future** object..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f784399",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_id = long_running_ray.remote(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4fcf3d0",
   "metadata": {},
   "source": [
    "...and get its value (synchronously):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecad1b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.get(task_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d22b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "task_ids = [long_running_ray.remote(i) for i in range(10)]\n",
    "ray.get(task_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a324db",
   "metadata": {},
   "source": [
    "## Exercise: Compute k nearest neighbours remotely"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f62167",
   "metadata": {},
   "source": [
    "We will reuse our definitions of kNN functions (slightly modified) and for random points fÃ­nd their neighbours:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b321460",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_K = 4   # How many nearest neighbors to consider\n",
    "\n",
    "def calculate_distances(query_points: np.ndarray, reference_points: np.ndarray, *, n_dim: int = 3) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Calculate mutual Euclidean distances between M query and N reference points.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    query_points: np.ndarray\n",
    "        (M, n_dim+) array of query points\n",
    "    reference_points: np.ndarray\n",
    "        (N, n_dim+) array of reference points\n",
    "    n_dim: int\n",
    "        Number of dimensions to consider (default: 3, for x, y, floor)\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    distances: np.ndarray\n",
    "        (M, N) array of the distances\n",
    "    \"\"\"\n",
    "    # Expand for broadcasting\n",
    "    query_points = query_points[:, np.newaxis,:n_dim]\n",
    "    reference_points = reference_points[np.newaxis, :, :n_dim]\n",
    "    return np.sqrt(np.sum((reference_points - query_points) ** 2, axis=-1))\n",
    "\n",
    "\n",
    "def knn_search(\n",
    "    query_points: np.ndarray,\n",
    "    reference_points: np.ndarray,\n",
    "    k: int,\n",
    "):\n",
    "    \"\"\"\n",
    "    Find k nearest neighbour reference point indices for N query points.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    indices: np.ndarray\n",
    "        (N, k) matrix of integral indices\n",
    "    \"\"\"\n",
    "    distances = calculate_distances(query_points, reference_points).T\n",
    "    return np.argpartition(distances, k, axis=0)[:k].T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc5527e",
   "metadata": {},
   "source": [
    "Let's create some points to test this on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc2af31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_random_points(\n",
    "    n_points: int, *, n_dim: int = 3, seed: int = 42\n",
    ") -> np.ndarray:\n",
    "    # TODO: Fix floor!\n",
    "    np.random.seed(seed)\n",
    "    return np.random.sample((n_points, n_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9382f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_points = create_random_points(5, seed=42)\n",
    "reference_points = create_random_points(10, seed=84)\n",
    "\n",
    "query_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9beca39",
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_distances(query_points, reference_points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce7aae9",
   "metadata": {},
   "source": [
    "First, we run this without ray (for sizable arrays):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d8086c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "query_points = create_random_points(32768, seed=42)\n",
    "reference_points = create_random_points(1000, seed=84)\n",
    "knn_search(query_points, reference_points, k=DEFAULT_K)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea75ba2",
   "metadata": {},
   "source": [
    "**Task**\n",
    "\n",
    "1. Change the knn_search function into a task.\n",
    "2. Submit the computation to ray and compare the results.\n",
    "3. Look at the execution times (and compare to local run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dec6640",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def knn_search_ray(\n",
    "    query_points: np.ndarray,\n",
    "    reference_points: np.ndarray,\n",
    "    k: int,\n",
    "):\n",
    "    \"\"\"\n",
    "    Find k nearest neighbour reference point indices for N query points.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    indices: np.ndarray\n",
    "        (N, k) matrix of integral indices\n",
    "    \"\"\"\n",
    "    distances = calculate_distances(query_points, reference_points).T\n",
    "    return np.argpartition(distances, k, axis=0)[:k].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29fdb21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "knn_id = knn_search_ray.remote(query_points, reference_points, k=DEFAULT_K)\n",
    "knn_results = ray.get(knn_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e276678",
   "metadata": {},
   "source": [
    "**Question:** Is there any improvement yet?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773e9ce7",
   "metadata": {},
   "source": [
    "## Monitoring ray"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18928aa6",
   "metadata": {},
   "source": [
    "Ray comes with a nice dashboard that allows you to observe running jobs. It runs in a local web server, mostly likely http://localhost:8265. This address is not accessible when running within Google Colab, and so you have to use a special trick to show a mini-window forwarded to the dashboard running in the cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3e252d",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_colab = \"google.colab\" in str(get_ipython())\n",
    "\n",
    "if is_colab:\n",
    "    from google.colab import output\n",
    "    output.serve_kernel_port_as_iframe(8265)  # The port may differ!\n",
    "else:\n",
    "    print(\"Not in google Colab. Try the local link, it should work.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff7e661",
   "metadata": {},
   "source": [
    "Now we submit something really huge:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6647a91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_points = create_random_points(10000000, seed=42)\n",
    "reference_points = create_random_points(100, seed=84)\n",
    "task_id = knn_search_ray.remote(query_points, reference_points, k=DEFAULT_K)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ae985f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ray.get(task_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9589eeba",
   "metadata": {},
   "source": [
    "## Exercise: Parallelize kNN execution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4534494f",
   "metadata": {},
   "source": [
    "Ray by itself does not do any optimisations or segments the tasks you send it. So you need to care about splitting the task yourself. Sometimes this is easy, sometimes it requires a deeper thought (or a library ;-)).\n",
    "\n",
    "If you want to improve the computation of nearest neighbours for a bunch of **independent** points, this is really easy (\"embarassingly parallel problem). Just take the 1000000 query points and split them into **batches**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de206ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_batches(array: np.ndarray, max_size: int) -> list[np.ndarray]:\n",
    "    \"\"\"\n",
    "    Split an array into smaller batches of a maximum size.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    array: np.ndarray\n",
    "        The array to split.\n",
    "    max_size: int\n",
    "        The maximum size of each batch.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    list[np.ndarray]\n",
    "        A list of arrays, each with a maximum size of `max_size`.\n",
    "    \"\"\"\n",
    "    return [array[i:i + max_size] for i in range(0, len(array), max_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28665191",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_into_batches(np.arange(10), 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d86a23c6",
   "metadata": {},
   "source": [
    "**Task**: \n",
    "1. Split the query points accordingly (you may experiment with batch_size, 1000 is pretty ok)\n",
    "2. Submit and retrieve the results for the batches\n",
    "3. Combine the results back into one array ([np.vstack](https://numpy.org/doc/stable/reference/generated/numpy.vstack.html) is your friend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983c0154",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_points = create_random_points(10000000, seed=42)\n",
    "reference_points = create_random_points(100, seed=84)\n",
    "\n",
    "batches = split_into_batches(query_points, 1000)\n",
    "batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b321737",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "task_ids = [\n",
    "    knn_search_ray.remote(batch, reference_points, k=DEFAULT_K) for batch in batches\n",
    "]\n",
    "knn_results = ray.get(task_ids)\n",
    "knn_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f671772c",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.vstack(knn_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0e5fc6",
   "metadata": {},
   "source": [
    "**Question** Can we parallelise in the reference points dimension?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8a7388",
   "metadata": {},
   "source": [
    "## Example: Predicting the housing prices with a database of reference points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d53937",
   "metadata": {},
   "source": [
    "We can use the distances to predict the housing prices (in arbitrary units, perhaps the monthly rent in $ for a single-bedroom apartment?) based on our custom regression implementation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b4561b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_prices(query_points, reference_points, k: int = DEFAULT_K):\n",
    "    \"\"\"\n",
    "    Find prices for N data_points.\n",
    "\n",
    "    Parameters:\n",
    "    query_points: np.ndarray\n",
    "        (N, 3) array of query points\n",
    "    reference_points: np.ndarray\n",
    "        (M, 4) array of data points with x, y, floor, and price\n",
    "    k: int\n",
    "        Number of nearest neighbors to consider\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    prices: np.ndarray\n",
    "        (N,) array of prices\n",
    "    \"\"\"\n",
    "    indices = knn_search(query_points, reference_points, k)\n",
    "    prices: np.ndarray = reference_points[indices, 3]\n",
    "    return prices.mean(axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767f7100",
   "metadata": {},
   "source": [
    "We have predefined reference points in an external file, modeled using an (unknown?) analytical function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2394045",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_colab = \"google.colab\" in str(get_ipython())\n",
    "\n",
    "# Download the data which are part of this repo\n",
    "if is_colab:\n",
    "    import urllib\n",
    "    url = \"https://github.com/coobas/europython-25/raw/refs/heads/main/data.parquet\"\n",
    "    urllib.request.urlretrieve(url, \"data.parquet\")\n",
    "    print(\"Data downloaded to data.parquet.\")\n",
    "else:\n",
    "    print(\"Not in Google Colab, skipping data download.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688d1d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_reference_points_df(path: Path = Path(\"../data.parquet\")) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Load reference data points from a parquet file.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    data_points: np.ndarray\n",
    "        (N, 4) array of data points with x, y, floor, and price columns\n",
    "    \"\"\"\n",
    "\n",
    "    return pd.read_parquet(path)\n",
    "    # return df[[\"x\", \"y\", \"floor\", \"price\"]].to_numpy().astype(float)\n",
    "\n",
    "reference_points_df = load_reference_points_df()\n",
    "reference_points_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb88b9e0",
   "metadata": {},
   "source": [
    "And we also want to run something against this. Let's start with random points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05185963",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_random_3d_points(\n",
    "        n_points: int = 100, *, ranges: list[tuple[float, float]] = [(-10, 10), (-10, 10), (1, 20)], ) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Create random 3D points within specified ranges.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    n_points: int\n",
    "        Number of points to generate.    \n",
    "    ranges: list[tuple[float, float]]\n",
    "        List of tuples specifying the range for each dimension.\n",
    "\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    points: np.ndarray\n",
    "        (n_points, len(ranges)) array of random points.\n",
    "    \"\"\"\n",
    "    if not ranges or len(ranges) != 3:\n",
    "        raise ValueError(\"Ranges must be a list of three tuples for x, y, and floor dimensions.\")\n",
    "    df = np.random.uniform(\n",
    "        low=[r[0] for r in ranges],\n",
    "        high=[r[1] for r in ranges],\n",
    "        size=(n_points, len(ranges))\n",
    "    )\n",
    "    df[:, 2] = df[:, 2].astype(int)  # Ensure the floor is an integer\n",
    "    return df\n",
    "\n",
    "query_points = create_random_3d_points()\n",
    "query_points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd4572d",
   "metadata": {},
   "source": [
    "Our query points need to live in the same dimension (i.e. x,y between -10 and 10, floor between 1 and 20). Note that the floor distibution probably is not \"uniform\", but for demonstration purposes, it should not matter that much."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9405e809",
   "metadata": {},
   "source": [
    "Let's see how the whole thing looks without ray:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b5b7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "prices = compute_prices(query_points, reference_points_df.to_numpy(), k=DEFAULT_K)\n",
    "prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291c06af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_points_and_prices(\n",
    "    query_points: np.ndarray, prices: np.ndarray\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Prepare human-friendly output from numpy arrays.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    query_points: np.ndarray\n",
    "        (N, 3) array of query points\n",
    "    prices: np.ndarray\n",
    "        (N,) array of prices\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    df: pd.DataFrame\n",
    "        DataFrame with columns x, y, floor, price\n",
    "    \"\"\"\n",
    "    return pd.DataFrame(\n",
    "        {\n",
    "            \"x\": query_points[:,0],\n",
    "            \"y\": query_points[:,1],\n",
    "            \"floor\": query_points[:,2].astype(int),\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08da8537",
   "metadata": {},
   "outputs": [],
   "source": [
    "combine_points_and_prices(query_points, prices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b25c8d",
   "metadata": {},
   "source": [
    "We already know how to parallelise this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70520155",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note we can apply the decorator directly to the function\n",
    "compute_prices_ray = ray.remote(compute_prices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ca9f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "query_points = create_random_3d_points(100_000)\n",
    "batches = split_into_batches(query_points, 1_000)\n",
    "task_ids = [\n",
    "    compute_prices_ray.remote(batch, reference_points_df.to_numpy(), k=DEFAULT_K) \n",
    "    for batch in batches\n",
    "]\n",
    "np.hstack(ray.get(task_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24025360",
   "metadata": {},
   "source": [
    "This is relatively fine but mind that we are passing the same reference points over and over again to ray (which requires repeated serialisation, ...). It does not matter that much in our case but if the reference points dataframe were larger, we might see a significant performance penalty."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a1f9e9",
   "metadata": {},
   "source": [
    "### Storing objects in ray"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954cf046",
   "metadata": {},
   "source": [
    "What we can do instead, is to pass the object to ray just once and pass its **object id**. Ray automatically cares about using the referenced value.\n",
    "\n",
    "Let's take it to the extreme and compute the price for each of 10,000 points in a separate task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4095cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "query_points = create_random_3d_points(10_000)\n",
    "batches = split_into_batches(query_points, 1_000)\n",
    "task_ids = [\n",
    "    compute_prices_ray.remote(batch, reference_points_df.to_numpy(), k=DEFAULT_K) \n",
    "    for batch in batches\n",
    "]\n",
    "ray.get(task_ids);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e2aaf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "query_points = create_random_3d_points(10_000)\n",
    "batches = split_into_batches(query_points, 1)\n",
    "task_ids = [\n",
    "    compute_prices_ray.remote(batch, reference_points_df.to_numpy(), k=DEFAULT_K) \n",
    "    for batch in batches\n",
    "]\n",
    "ray.get(task_ids);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cddff209",
   "metadata": {},
   "source": [
    "Do you see the degraded performance?\n",
    "\n",
    "Of course it is mostly due to creating a task per point but part of the penalty is coming from passing the array so many times."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f861ca",
   "metadata": {},
   "source": [
    "Putting objects into a ray cluster is very simple, actually:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a936976e",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 9\n",
    "arg_id = ray.put(i)\n",
    "arg_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f3553a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.get(long_running_ray.remote(arg_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a44f6de",
   "metadata": {},
   "source": [
    "**Task**\n",
    "\n",
    "1. Modify the `compute_prices_ray` from above so that `reference_points_df.to_numpy()` is stored in the ray cluster and reused in each task.\n",
    "2. Compare the execution time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b3524c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "query_points = create_random_3d_points(10000)\n",
    "batches = split_into_batches(query_points, 1)\n",
    "reference_id = ray.put(reference_points_df.to_numpy())\n",
    "task_ids = [\n",
    "    compute_prices_ray.remote(batch, reference_id, k=DEFAULT_K) \n",
    "    for batch in batches\n",
    "]\n",
    "ray.get(task_ids);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da39ad63",
   "metadata": {},
   "source": [
    "Is it better now?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872b6700",
   "metadata": {},
   "source": [
    "**Question**: Would putting query points (either whole or batched) to ray help as well?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc8044f",
   "metadata": {},
   "source": [
    "## TODO: Optional: Actors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d189a6d8",
   "metadata": {},
   "source": [
    "We would like to create a map of prices, i.e. sample data in a grid over the allowed area and use our kNN model to predict a price for each of those (given the floor as a parameter). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "8ecde9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PriceMap:\n",
    "    DEFAULT_N_POINTS = 10\n",
    "    DEFAULT_LIMIT = 10\n",
    "    DEFAULT_BATCH_SIZE = 1000\n",
    "\n",
    "    def __init__(self, reference_points: np.ndarray, *, floor: int = 1, n_points: int = DEFAULT_N_POINTS):\n",
    "        self.floor = floor\n",
    "        self.n_points = n_points\n",
    "        self.reference_points = reference_points\n",
    "        self.limit = self.DEFAULT_LIMIT\n",
    "        self.create_query_points()\n",
    "\n",
    "    def create_query_points(self) -> None:\n",
    "        \"\"\"\n",
    "        Create a homogenous grid of points to create a map.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        query_points: np.ndarray\n",
    "            (n_points x n_points, 3) array of query points\n",
    "        \"\"\"\n",
    "        x = np.linspace(-self.limit, self.limit, self.n_points)\n",
    "        y = np.linspace(-self.limit, self.limit, self.n_points)\n",
    "        x, y = np.meshgrid(x, y)\n",
    "        self.query_points = np.vstack([x.flatten(), y.flatten(), np.ones(x.flatten().shape[0]) * self.floor]).T\n",
    "    \n",
    "    def compute_prices(self) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Compute prices for the query points based on the reference points.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        prices: np.ndarray\n",
    "            (n_points x n_points,) array of prices for each query point\n",
    "        \"\"\"\n",
    "        return compute_prices(self.query_points, self.reference_points, k=DEFAULT_K)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f4a8c01d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.62 s, sys: 295 ms, total: 1.92 s\n",
      "Wall time: 1.93 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([502.62571959, 502.62571959, 502.62571959, ..., 497.50754281,\n",
       "       497.50167414, 497.50167414], shape=(10000,))"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "map = PriceMap(reference_points_df.to_numpy(), floor=1, n_points=100)\n",
    "map.compute_prices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff4a56e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 20.4 ms, sys: 4.72 ms, total: 25.1 ms\n",
      "Wall time: 2.45 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([502.62571959, 502.62571959, 502.62571959, ..., 497.50754281,\n",
       "       497.50167414, 497.50167414], shape=(10000,))"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "@ray.remote\n",
    "class MapActor(PriceMap):\n",
    "    ...\n",
    "    \n",
    "\n",
    "map_actor = MapActor.remote(reference_points_df.to_numpy(), floor=1, n_points=100)\n",
    "prices_id = map_actor.compute_prices.remote()\n",
    "ray.get(prices_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "0bfbc464",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 711 ms, sys: 258 ms, total: 969 ms\n",
      "Wall time: 2min 36s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([502.62571959, 502.62571959, 502.62571959, ..., 497.50754281,\n",
       "       497.50167414, 497.50167414], shape=(10000,))"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "@ray.remote\n",
    "class MapBatchActor(PriceMap):\n",
    "    ... \n",
    "    \n",
    "    def compute_prices(self):\n",
    "        \"\"\"\n",
    "        Compute prices for the query points based on the reference points.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        prices: np.ndarray\n",
    "            (n_points x n_points,) array of prices for each query point\n",
    "        \"\"\"\n",
    "        prices = [\n",
    "            compute_prices_ray.remote(batch, self.reference_points, k=DEFAULT_K) \n",
    "            for batch in split_into_batches(self.query_points, self.DEFAULT_BATCH_SIZE)\n",
    "        ]\n",
    "        return np.hstack(ray.get(prices))\n",
    "    \n",
    "map_actor = MapBatchActor.remote(reference_points_df.to_numpy(), floor=1, n_points=100)\n",
    "prices_id = map_actor.compute_prices.remote()\n",
    "ray.get(prices_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eee7b93",
   "metadata": {},
   "source": [
    "# TODO: Check that this works in Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "f29b7737",
   "metadata": {},
   "outputs": [
    {
     "ename": "RayTaskError(RuntimeError)",
     "evalue": "\u001b[36mray::MapJaxBatchActor.compute_prices()\u001b[39m (pid=150796, ip=192.168.1.23, actor_id=ebede1e1a65b7accf6c7e74401000000, repr=<__main__.MapJaxBatchActor object at 0x7f5288a96f60>)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/tmp/ipykernel_119217/294526014.py\", line 34, in compute_prices\n           ^^^^^^^^^^^^^^^^^^^\n           ^^^^^^^^^^^^^^^^^^^^^\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nray.exceptions.RayTaskError(RuntimeError): \u001b[36mray::compute_prices_jax_ray()\u001b[39m (pid=119973, ip=192.168.1.23)\n              ^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/jan/code/collaboration/europython-25/.venv/lib/python3.12/site-packages/jax/_src/xla_bridge.py\", line 943, in _init_backend\n    backend = registration.factory()\n              ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/jan/code/collaboration/europython-25/.venv/lib/python3.12/site-packages/jax/_src/xla_bridge.py\", line 633, in factory\n    return xla_client.make_c_api_client(plugin_name, updated_options, None)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/jan/code/collaboration/europython-25/.venv/lib/python3.12/site-packages/jaxlib/xla_client.py\", line 159, in make_c_api_client\n    return _xla.get_c_api_client(plugin_name, options, distributed_client)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\njaxlib.xla_extension.XlaRuntimeError: FAILED_PRECONDITION: No visible GPU devices.\n\nDuring handling of the above exception, another exception occurred:\n\n\u001b[36mray::compute_prices_jax_ray()\u001b[39m (pid=119973, ip=192.168.1.23)\n  File \"/tmp/ipykernel_119217/294526014.py\", line 12, in compute_prices_jax_ray\n  File \"/tmp/ipykernel_119217/294526014.py\", line 7, in calculate_distances_jax\n  File \"/home/jan/code/collaboration/europython-25/.venv/lib/python3.12/site-packages/jax/_src/numpy/reductions.py\", line 310, in sum\n    return _reduce_sum(a, axis=_ensure_optional_axes(axis), dtype=dtype, out=out,\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nRuntimeError: Unable to initialize backend 'cuda': FAILED_PRECONDITION: No visible GPU devices. (you may need to uninstall the failing plugin package, or set JAX_PLATFORMS=cpu to skip this backend.)\n--------------------\nFor simplicity, JAX has removed its internal frames from the traceback of the following exception. Set JAX_TRACEBACK_FILTERING=off to include these.\n--------------------\nFor simplicity, JAX has removed its internal frames from the traceback of the following exception. Set JAX_TRACEBACK_FILTERING=off to include these.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRayTaskError(RuntimeError)\u001b[39m                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[87]\u001b[39m\u001b[32m, line 38\u001b[39m\n\u001b[32m     36\u001b[39m map_actor = MapJaxBatchActor.remote(reference_points_df.to_numpy(), floor=\u001b[32m1\u001b[39m, n_points=\u001b[32m5\u001b[39m)\n\u001b[32m     37\u001b[39m prices_id = map_actor.compute_prices.remote()\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m \u001b[43mray\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprices_id\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/collaboration/europython-25/.venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py:21\u001b[39m, in \u001b[36mwrap_auto_init.<locals>.auto_init_wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(fn)\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mauto_init_wrapper\u001b[39m(*args, **kwargs):\n\u001b[32m     20\u001b[39m     auto_init_ray()\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/collaboration/europython-25/.venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py:103\u001b[39m, in \u001b[36mclient_mode_hook.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    101\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m func.\u001b[34m__name__\u001b[39m != \u001b[33m\"\u001b[39m\u001b[33minit\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m is_client_mode_enabled_by_default:\n\u001b[32m    102\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(ray, func.\u001b[34m__name__\u001b[39m)(*args, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/collaboration/europython-25/.venv/lib/python3.12/site-packages/ray/_private/worker.py:2822\u001b[39m, in \u001b[36mget\u001b[39m\u001b[34m(object_refs, timeout)\u001b[39m\n\u001b[32m   2816\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   2817\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInvalid type of object refs, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(object_refs)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, is given. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2818\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[33mobject_refs\u001b[39m\u001b[33m'\u001b[39m\u001b[33m must either be an ObjectRef or a list of ObjectRefs. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2819\u001b[39m     )\n\u001b[32m   2821\u001b[39m \u001b[38;5;66;03m# TODO(ujvl): Consider how to allow user to retrieve the ready objects.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2822\u001b[39m values, debugger_breakpoint = \u001b[43mworker\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_objects\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobject_refs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2823\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, value \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(values):\n\u001b[32m   2824\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, RayError):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/collaboration/europython-25/.venv/lib/python3.12/site-packages/ray/_private/worker.py:930\u001b[39m, in \u001b[36mWorker.get_objects\u001b[39m\u001b[34m(self, object_refs, timeout, return_exceptions, skip_deserialization)\u001b[39m\n\u001b[32m    928\u001b[39m     global_worker.core_worker.dump_object_store_memory_usage()\n\u001b[32m    929\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, RayTaskError):\n\u001b[32m--> \u001b[39m\u001b[32m930\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m value.as_instanceof_cause()\n\u001b[32m    931\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    932\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m value\n",
      "\u001b[31mRayTaskError(RuntimeError)\u001b[39m: \u001b[36mray::MapJaxBatchActor.compute_prices()\u001b[39m (pid=150796, ip=192.168.1.23, actor_id=ebede1e1a65b7accf6c7e74401000000, repr=<__main__.MapJaxBatchActor object at 0x7f5288a96f60>)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/tmp/ipykernel_119217/294526014.py\", line 34, in compute_prices\n           ^^^^^^^^^^^^^^^^^^^\n           ^^^^^^^^^^^^^^^^^^^^^\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nray.exceptions.RayTaskError(RuntimeError): \u001b[36mray::compute_prices_jax_ray()\u001b[39m (pid=119973, ip=192.168.1.23)\n              ^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/jan/code/collaboration/europython-25/.venv/lib/python3.12/site-packages/jax/_src/xla_bridge.py\", line 943, in _init_backend\n    backend = registration.factory()\n              ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/jan/code/collaboration/europython-25/.venv/lib/python3.12/site-packages/jax/_src/xla_bridge.py\", line 633, in factory\n    return xla_client.make_c_api_client(plugin_name, updated_options, None)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/jan/code/collaboration/europython-25/.venv/lib/python3.12/site-packages/jaxlib/xla_client.py\", line 159, in make_c_api_client\n    return _xla.get_c_api_client(plugin_name, options, distributed_client)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\njaxlib.xla_extension.XlaRuntimeError: FAILED_PRECONDITION: No visible GPU devices.\n\nDuring handling of the above exception, another exception occurred:\n\n\u001b[36mray::compute_prices_jax_ray()\u001b[39m (pid=119973, ip=192.168.1.23)\n  File \"/tmp/ipykernel_119217/294526014.py\", line 12, in compute_prices_jax_ray\n  File \"/tmp/ipykernel_119217/294526014.py\", line 7, in calculate_distances_jax\n  File \"/home/jan/code/collaboration/europython-25/.venv/lib/python3.12/site-packages/jax/_src/numpy/reductions.py\", line 310, in sum\n    return _reduce_sum(a, axis=_ensure_optional_axes(axis), dtype=dtype, out=out,\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nRuntimeError: Unable to initialize backend 'cuda': FAILED_PRECONDITION: No visible GPU devices. (you may need to uninstall the failing plugin package, or set JAX_PLATFORMS=cpu to skip this backend.)\n--------------------\nFor simplicity, JAX has removed its internal frames from the traceback of the following exception. Set JAX_TRACEBACK_FILTERING=off to include these.\n--------------------\nFor simplicity, JAX has removed its internal frames from the traceback of the following exception. Set JAX_TRACEBACK_FILTERING=off to include these.",
      "--------------------\nFor simplicity, JAX has removed its internal frames from the traceback of the following exception. Set JAX_TRACEBACK_FILTERING=off to include these."
     ]
    }
   ],
   "source": [
    "import jax.numpy as jnp\n",
    "\n",
    "\n",
    "def calculate_distances_jax(query_points: jnp.ndarray, reference_points: jnp.ndarray, *, n_dim: int = 3) -> jnp.ndarray:\n",
    "    query_points = query_points[:, jnp.newaxis, :n_dim]\n",
    "    reference_points = reference_points[jnp.newaxis, :, :n_dim]\n",
    "    return jnp.sqrt(jnp.sum((reference_points - query_points) ** 2, axis=-1))\n",
    "\n",
    "\n",
    "@ray.remote\n",
    "def compute_prices_jax_ray(query_points, reference_points, k=DEFAULT_K):\n",
    "    distances = calculate_distances_jax(query_points, reference_points).T\n",
    "    indices = jnp.argpartition(distances, k, axis=0)[:k].T\n",
    "    return jnp.mean(reference_points[indices, 3], axis=1)\n",
    "\n",
    "\n",
    "@ray.remote\n",
    "class MapJaxBatchActor(PriceMap):\n",
    "    ...\n",
    "\n",
    "    def compute_prices(self):\n",
    "        \"\"\"\n",
    "        Compute prices for the query points based on the reference points.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        prices: np.ndarray\n",
    "            (n_points x n_points,) array of prices for each query point\n",
    "        \"\"\"\n",
    "        prices = [\n",
    "            compute_prices_jax_ray.remote(batch, self.reference_points, k=DEFAULT_K) \n",
    "            for batch in split_into_batches(self.query_points, self.DEFAULT_BATCH_SIZE)\n",
    "        ]\n",
    "        return jnp.hstack(ray.get(prices))\n",
    "\n",
    "map_actor = MapJaxBatchActor.remote(reference_points_df.to_numpy(), floor=1, n_points=5)\n",
    "prices_id = map_actor.compute_prices.remote()\n",
    "ray.get(prices_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed336d8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aa9309eb",
   "metadata": {},
   "source": [
    "TODO: Create an actor?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3607f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_points(points: np.ndarray) -> None:\n",
    "    \"\"\"\n",
    "    Draw points on a map.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    points: np.ndarray\n",
    "        (N, 3) array of points to draw\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame({\"x\": points[:,0], \"y\": points[:,1]})\n",
    "    fig = px.scatter(df, x=\"x\", y=\"y\", title=\"Query points\")\n",
    "    fig.show()\n",
    "\n",
    "draw_points(create_query_points(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1511b6",
   "metadata": {},
   "source": [
    "## TODO: Optional: ray + jax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bddb47f",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "accelerating-scientific-python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
