{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3497647e",
   "metadata": {},
   "source": [
    "# Parallel computation with Ray"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54735612",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/coobas/europython-25/blob/main/solutions/04-ray.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0bf1e03",
   "metadata": {},
   "source": [
    "[Ray](https://docs.ray.io/en/latest/index.html) is a set of libraries that (among others) allow an easy parallelisation of Python tasks - both locally and also in clusters. This part is called **Ray Core**.\n",
    "\n",
    "Apart from this, it also provides specialised libraries for data processing (**Ray Data**), for machine learning and even reinforcement learning (**Ray Train**, **Ray Train**, ...). We will not deal with those in this workshop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8631bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this in Google Collab, not needed if you install this package locally\n",
    "!pip install ray[default]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8faadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obligatory imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ray\n",
    "from typing import Optional\n",
    "import plotly.express as px\n",
    "from IPython import get_ipython\n",
    "\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f108ee",
   "metadata": {},
   "source": [
    "Let's create something that takes long to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e000c854",
   "metadata": {},
   "outputs": [],
   "source": [
    "def long_running(i: int) -> int:\n",
    "    \"\"\"A long running task that we will parallelise.\"\"\"\n",
    "    import time\n",
    "    time.sleep(1)\n",
    "    return i * i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2c7a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "long_running(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79f70f5",
   "metadata": {},
   "source": [
    "If we run ten such tasks, they take... 10 times as long to execute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc67e736",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "[long_running(i) for i in range(10)]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49eab1e0",
   "metadata": {},
   "source": [
    "If these tasks are independent, we can run them in parallel. There are of course options in Python itself:\n",
    "\n",
    "- [multiprocessing](https://docs.python.org/3.13/library/multiprocessing.html) and [concurrent.futures](https://docs.python.org/3.13/library/concurrent.futures.html)\n",
    "- threading (with GIL-releasing code or with caution in free-threaded Python 3.13+)\n",
    "\n",
    "This does scale exactly well if there are more tasks than CPUs / GPUs on your machine...\n",
    "\n",
    "Other options:\n",
    "- [celery](https://docs.celeryq.dev/en/stable/)\n",
    "- [dask](https://docs.dask.org/en/stable/index.html)\n",
    "\n",
    "With their strengths and weaknesses. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a5e9c9",
   "metadata": {},
   "source": [
    "## Use ray"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6e34b8",
   "metadata": {},
   "source": [
    "Ray always runs a server (even implicitly) and executes the task in nodes that it manages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11fc3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07305b3",
   "metadata": {},
   "source": [
    "This either connects to an existing local server, or creates a new one. (In the same way, we can connect to a different one)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00e82ff",
   "metadata": {},
   "source": [
    "### Remote functions\n",
    "\n",
    "Any function we decorate with the `ray.remote` decorator, becomes a **task** that can be submitted to this server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55769580",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def long_running_ray(i: int) -> int:\n",
    "    import time\n",
    "    time.sleep(1)\n",
    "    return i * i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8dff515",
   "metadata": {},
   "outputs": [],
   "source": [
    "long_running_ray(1)  # This will fail"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db49b322",
   "metadata": {},
   "source": [
    "Well, the error message is right. We should use `.remote` (a different one!)\n",
    "\n",
    "Better (and see that we pass arguments the same way):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2e9046",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "long_running_ray.remote(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ebce590",
   "metadata": {},
   "source": [
    "What happened? The task was submitted to the cluster. But asynchronously. We have to capture the **future** object..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f784399",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_id = long_running_ray.remote(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4fcf3d0",
   "metadata": {},
   "source": [
    "...and get its value (synchronously):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecad1b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.get(task_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d22b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "task_ids = [long_running_ray.remote(i) for i in range(10)]\n",
    "ray.get(task_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a324db",
   "metadata": {},
   "source": [
    "## Exercise: Compute k nearest neighbours remotely"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f62167",
   "metadata": {},
   "source": [
    "We will reuse our definitions of kNN functions (slightly modified) and for random points fÃ­nd their neighbours:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b321460",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_K = 4   # How many nearest neighbors to consider\n",
    "\n",
    "def calculate_distances(query_points: np.ndarray, reference_points: np.ndarray, *, n_dim: int = 3) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Calculate mutual Euclidean distances between M query and N reference points.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    query_points: np.ndarray\n",
    "        (M, n_dim+) array of query points\n",
    "    reference_points: np.ndarray\n",
    "        (N, n_dim+) array of reference points\n",
    "    n_dim: int\n",
    "        Number of dimensions to consider (default: 3, for x, y, floor)\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    distances: np.ndarray\n",
    "        (M, N) array of the distances\n",
    "    \"\"\"\n",
    "    # Expand for broadcasting\n",
    "    query_points = query_points[:, np.newaxis,:n_dim]\n",
    "    reference_points = reference_points[np.newaxis, :, :n_dim]\n",
    "    return np.sqrt(np.sum((reference_points - query_points) ** 2, axis=-1))\n",
    "\n",
    "\n",
    "def knn_search(\n",
    "    query_points: np.ndarray,\n",
    "    reference_points: np.ndarray,\n",
    "    k: int,\n",
    "):\n",
    "    \"\"\"\n",
    "    Find k nearest neighbour reference point indices for N query points.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    indices: np.ndarray\n",
    "        (N, k) matrix of integral indices\n",
    "    \"\"\"\n",
    "    distances = calculate_distances(query_points, reference_points).T\n",
    "    return np.argpartition(distances, k, axis=0)[:k].T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc5527e",
   "metadata": {},
   "source": [
    "Let's create some points to test this on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc2af31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_random_points(\n",
    "    n_points: int, *, n_dim: int = 3, seed: int = 42\n",
    ") -> np.ndarray:\n",
    "    # TODO: Fix floor!\n",
    "    np.random.seed(seed)\n",
    "    return np.random.sample((n_points, n_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9382f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_points = create_random_points(5, seed=42)\n",
    "reference_points = create_random_points(10, seed=84)\n",
    "\n",
    "query_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9beca39",
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_distances(query_points, reference_points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce7aae9",
   "metadata": {},
   "source": [
    "First, we run this without ray (for sizable arrays):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d8086c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "query_points = create_random_points(32768, seed=42)\n",
    "reference_points = create_random_points(1000, seed=84)\n",
    "knn_search(query_points, reference_points, k=DEFAULT_K)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea75ba2",
   "metadata": {},
   "source": [
    "**Task**\n",
    "\n",
    "1. Change the knn_search function into a task.\n",
    "2. Submit the computation to ray and compare the results.\n",
    "3. Look at the execution times (and compare to local run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dec6640",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def knn_search_ray(\n",
    "    query_points: np.ndarray,\n",
    "    reference_points: np.ndarray,\n",
    "    k: int,\n",
    "):\n",
    "    \"\"\"\n",
    "    Find k nearest neighbour reference point indices for N query points.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    indices: np.ndarray\n",
    "        (N, k) matrix of integral indices\n",
    "    \"\"\"\n",
    "    distances = calculate_distances(query_points, reference_points).T\n",
    "    return np.argpartition(distances, k, axis=0)[:k].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29fdb21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "knn_id = knn_search_ray.remote(query_points, reference_points, k=DEFAULT_K)\n",
    "knn_results = ray.get(knn_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e276678",
   "metadata": {},
   "source": [
    "**Question:** Is there any improvement yet?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773e9ce7",
   "metadata": {},
   "source": [
    "## Monitoring ray"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18928aa6",
   "metadata": {},
   "source": [
    "Ray comes with a nice dashboard that allows you to observe running jobs. It runs in a local web server, mostly likely http://localhost:8265. This address is not accessible when running within Google Colab, and so you have to use a special trick to show a mini-window forwarded to the dashboard running in the cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3e252d",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_colab = \"google.colab\" in str(get_ipython())\n",
    "\n",
    "if is_colab:\n",
    "    from google.colab import output\n",
    "    output.serve_kernel_port_as_iframe(8265)  # The port may differ!\n",
    "else:\n",
    "    print(\"Not in google Colab. Try the local link, it should work.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff7e661",
   "metadata": {},
   "source": [
    "Now we submit something really huge:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6647a91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_points = create_random_points(1_000_000, seed=42)\n",
    "reference_points = create_random_points(100, seed=84)\n",
    "task_id = knn_search_ray.remote(query_points, reference_points, k=DEFAULT_K)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ae985f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ray.get(task_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9589eeba",
   "metadata": {},
   "source": [
    "## Exercise: Parallelize kNN execution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4534494f",
   "metadata": {},
   "source": [
    "[Ray Core](https://docs.ray.io/en/latest/ray-core/walkthrough.html) by itself does not do any optimisations or segments the tasks you send it. So you need to care about splitting the task yourself. Sometimes this is easy, sometimes it requires a deeper thought (or a library ;-)). There is also [Ray Data](https://docs.ray.io/en/latest/data/data.html) that can help with this but here we demonstrate a simple approach using just Ray Core.\n",
    "\n",
    "If you want to improve the computation of nearest neighbours for a bunch of **independent** points, this is really easy (\"embarassingly parallel problem). Just take the 1000000 query points and split them into **batches**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de206ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_batches(array: np.ndarray, max_size: int) -> list[np.ndarray]:\n",
    "    \"\"\"\n",
    "    Split an array into smaller batches of a maximum size.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    array: np.ndarray\n",
    "        The array to split.\n",
    "    max_size: int\n",
    "        The maximum size of each batch.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    list[np.ndarray]\n",
    "        A list of arrays, each with a maximum size of `max_size`.\n",
    "    \"\"\"\n",
    "    return [array[i:i + max_size] for i in range(0, len(array), max_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28665191",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_into_batches(np.arange(10), 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d86a23c6",
   "metadata": {},
   "source": [
    "**Task**: \n",
    "1. Split the query points accordingly (you may experiment with batch_size, 1000 is pretty ok)\n",
    "2. Submit and retrieve the results for the batches\n",
    "3. Combine the results back into one array ([np.vstack](https://numpy.org/doc/stable/reference/generated/numpy.vstack.html) is your friend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983c0154",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_points = create_random_points(1_000_000, seed=42)\n",
    "reference_points = create_random_points(100, seed=84)\n",
    "\n",
    "batches = split_into_batches(query_points, 1000)\n",
    "print(f\"{len(batches)} batches\")\n",
    "print(\"First batch:\")\n",
    "print(batches[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b321737",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "task_ids = [\n",
    "    knn_search_ray.remote(batch, reference_points, k=DEFAULT_K) for batch in batches\n",
    "]\n",
    "knn_results = ray.get(task_ids)\n",
    "knn_results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f671772c",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.vstack(knn_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0e5fc6",
   "metadata": {},
   "source": [
    "**Question** Can we parallelise in the reference points dimension?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8a7388",
   "metadata": {},
   "source": [
    "## Example: Predicting the housing prices with a database of reference points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d53937",
   "metadata": {},
   "source": [
    "We can use the distances to predict the housing prices (in arbitrary units, perhaps the monthly rent in $ for a single-bedroom apartment?) based on our custom regression implementation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b4561b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_prices(query_points, reference_points, k: int = DEFAULT_K):\n",
    "    \"\"\"\n",
    "    Find prices for N data_points.\n",
    "\n",
    "    Parameters:\n",
    "    query_points: np.ndarray\n",
    "        (N, 3) array of query points\n",
    "    reference_points: np.ndarray\n",
    "        (M, 4) array of data points with x, y, floor, and price\n",
    "    k: int\n",
    "        Number of nearest neighbors to consider\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    prices: np.ndarray\n",
    "        (N,) array of prices\n",
    "    \"\"\"\n",
    "    indices = knn_search(query_points, reference_points, k)\n",
    "    prices: np.ndarray = reference_points[indices, 3]\n",
    "    return prices.mean(axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767f7100",
   "metadata": {},
   "source": [
    "We have predefined reference points in an external file, modeled using an (unknown?) analytical function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2394045",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_colab = \"google.colab\" in str(get_ipython())\n",
    "\n",
    "# Download the data which are part of this repo\n",
    "if is_colab:\n",
    "    import urllib\n",
    "    url = \"https://github.com/coobas/europython-25/raw/refs/heads/main/data.parquet\"\n",
    "    urllib.request.urlretrieve(url, \"data.parquet\")\n",
    "    print(\"Data downloaded to data.parquet.\")\n",
    "else:\n",
    "    print(\"Not in Google Colab, skipping data download.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688d1d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_reference_points_df(path: Path = Path(\"../data.parquet\")) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Load reference data points from a parquet file.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    data_points: np.ndarray\n",
    "        (N, 4) array of data points with x, y, floor, and price columns\n",
    "    \"\"\"\n",
    "\n",
    "    return pd.read_parquet(path)\n",
    "    # return df[[\"x\", \"y\", \"floor\", \"price\"]].to_numpy().astype(float)\n",
    "\n",
    "reference_points_df = load_reference_points_df()\n",
    "reference_points_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb88b9e0",
   "metadata": {},
   "source": [
    "And we also want to run something against this. Let's start with random points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05185963",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_random_3d_points(\n",
    "        n_points: int = 100, *, ranges: list[tuple[float, float]] = [(-10, 10), (-10, 10), (1, 20)], ) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Create random 3D points within specified ranges.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    n_points: int\n",
    "        Number of points to generate.    \n",
    "    ranges: list[tuple[float, float]]\n",
    "        List of tuples specifying the range for each dimension.\n",
    "\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    points: np.ndarray\n",
    "        (n_points, len(ranges)) array of random points.\n",
    "    \"\"\"\n",
    "    if not ranges or len(ranges) != 3:\n",
    "        raise ValueError(\"Ranges must be a list of three tuples for x, y, and floor dimensions.\")\n",
    "    df = np.random.uniform(\n",
    "        low=[r[0] for r in ranges],\n",
    "        high=[r[1] for r in ranges],\n",
    "        size=(n_points, len(ranges))\n",
    "    )\n",
    "    df[:, 2] = df[:, 2].astype(int)  # Ensure the floor is an integer\n",
    "    return df\n",
    "\n",
    "query_points = create_random_3d_points()\n",
    "print(f\"Query points array, shape: {query_points.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd4572d",
   "metadata": {},
   "source": [
    "Our query points need to live in the same dimension (i.e. x,y between -10 and 10, floor between 1 and 20). Note that the floor distibution probably is not \"uniform\", but for demonstration purposes, it should not matter that much."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9405e809",
   "metadata": {},
   "source": [
    "Let's see how the whole thing looks without ray:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b5b7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "prices = compute_prices(query_points, reference_points_df.to_numpy(), k=DEFAULT_K)\n",
    "prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291c06af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_points_and_prices(\n",
    "    query_points: np.ndarray, prices: np.ndarray\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Prepare human-friendly output from numpy arrays.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    query_points: np.ndarray\n",
    "        (N, 3) array of query points\n",
    "    prices: np.ndarray\n",
    "        (N,) array of prices\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    df: pd.DataFrame\n",
    "        DataFrame with columns x, y, floor, price\n",
    "    \"\"\"\n",
    "    return pd.DataFrame(\n",
    "        {\n",
    "            \"x\": query_points[:,0],\n",
    "            \"y\": query_points[:,1],\n",
    "            \"floor\": query_points[:,2].astype(int),\n",
    "            \"price\": prices\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08da8537",
   "metadata": {},
   "outputs": [],
   "source": [
    "combine_points_and_prices(query_points, prices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b25c8d",
   "metadata": {},
   "source": [
    "We already know how to parallelise this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70520155",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note we can apply the decorator directly to the function\n",
    "compute_prices_ray = ray.remote(compute_prices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ca9f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "query_points = create_random_3d_points(100_000)\n",
    "batches = split_into_batches(query_points, 1_000)\n",
    "task_ids = [\n",
    "    compute_prices_ray.remote(batch, reference_points_df.to_numpy(), k=DEFAULT_K) \n",
    "    for batch in batches\n",
    "]\n",
    "prices = np.hstack(ray.get(task_ids))\n",
    "combine_points_and_prices(\n",
    "    query_points,\n",
    "    prices\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24025360",
   "metadata": {},
   "source": [
    "This is relatively fine but mind that we are passing the same reference points over and over again to ray (which requires repeated serialisation, ...). It does not matter that much in our case but if the reference points dataframe were larger, we might see a significant performance penalty."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a1f9e9",
   "metadata": {},
   "source": [
    "### Storing objects in ray"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954cf046",
   "metadata": {},
   "source": [
    "What we can do instead, is to pass the object to ray just once and pass its **object id**. Ray automatically cares about using the referenced value.\n",
    "\n",
    "Let's take it to the extreme and compute the price for each of 10,000 points in a separate task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4095cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "query_points = create_random_3d_points(10_000)\n",
    "batches = split_into_batches(query_points, 1_000)\n",
    "task_ids = [\n",
    "    compute_prices_ray.remote(batch, reference_points_df.to_numpy(), k=DEFAULT_K) \n",
    "    for batch in batches\n",
    "]\n",
    "ray.get(task_ids);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e2aaf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "query_points = create_random_3d_points(10_000)\n",
    "batches = split_into_batches(query_points, 1)\n",
    "task_ids = [\n",
    "    compute_prices_ray.remote(batch, reference_points_df.to_numpy(), k=DEFAULT_K) \n",
    "    for batch in batches\n",
    "]\n",
    "ray.get(task_ids);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cddff209",
   "metadata": {},
   "source": [
    "Do you see the degraded performance?\n",
    "\n",
    "Of course it is mostly due to creating a task per point but part of the penalty is coming from passing the array so many times."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f861ca",
   "metadata": {},
   "source": [
    "Putting objects into a ray cluster is very simple, actually:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a936976e",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 9\n",
    "arg_id = ray.put(i)\n",
    "arg_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f3553a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.get(long_running_ray.remote(arg_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a44f6de",
   "metadata": {},
   "source": [
    "**Task**\n",
    "\n",
    "1. Modify the `compute_prices_ray` from above so that `reference_points_df.to_numpy()` is stored in the ray cluster and reused in each task.\n",
    "2. Compare the execution time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b3524c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "query_points = create_random_3d_points(10000)\n",
    "batches = split_into_batches(query_points, 1)\n",
    "reference_id = ray.put(reference_points_df.to_numpy())\n",
    "task_ids = [\n",
    "    compute_prices_ray.remote(batch, reference_id, k=DEFAULT_K) \n",
    "    for batch in batches\n",
    "]\n",
    "ray.get(task_ids);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da39ad63",
   "metadata": {},
   "source": [
    "Is it better now?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872b6700",
   "metadata": {},
   "source": [
    "**Question**: Would putting query points (either whole or batched) to ray help as well?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc8044f",
   "metadata": {},
   "source": [
    "## Optional: Actors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d189a6d8",
   "metadata": {},
   "source": [
    "We would like to create a map of prices, i.e. sample data in a grid over the allowed area and use our kNN model to predict a price for each of those (given the floor as a parameter). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c69455",
   "metadata": {},
   "source": [
    "# TODO: Simple actor intro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4a0515",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PriceMap:\n",
    "    DEFAULT_POINTS_PER_DIM = 21\n",
    "    DEFAULT_LIMIT = 10.0\n",
    "    DEFAULT_FLOOR = 1\n",
    "\n",
    "    points_per_dim: int\n",
    "    limit: float\n",
    "    floor: int\n",
    "\n",
    "    query_points: np.ndarray\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        reference_points: np.ndarray,\n",
    "        points_per_dim: int = DEFAULT_POINTS_PER_DIM,\n",
    "        floor: int = DEFAULT_FLOOR,\n",
    "        limit: float = DEFAULT_LIMIT,\n",
    "    ):\n",
    "        self.points_per_dim = points_per_dim\n",
    "        self.reference_points = reference_points\n",
    "        self.floor = floor\n",
    "        self.limit = limit\n",
    "        self._create_query_points(floor=floor)\n",
    "\n",
    "    def _create_query_points(self, *, floor: int) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Create a homogenous grid of points with a floor to create a map.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        query_points: np.ndarray\n",
    "            (n_points x n_points, 3) array of query points\n",
    "        \"\"\"\n",
    "        x = np.linspace(-self.limit, self.limit, self.points_per_dim)\n",
    "        y = np.linspace(-self.limit, self.limit, self.points_per_dim)\n",
    "        x, y = (arr.flatten() for arr in np.meshgrid(x, y))\n",
    "        self.query_points = np.vstack([x, y, np.ones(x.shape[0]) * floor]).T\n",
    "\n",
    "    def compute_prices(self) -> np.ndarray:\n",
    "        return compute_prices(self.query_points, self.reference_points, k=DEFAULT_K)\n",
    "    \n",
    "    def compute_prices_df(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Compute prices and return as a DataFrame.\n",
    "        \"\"\"\n",
    "        prices = self.compute_prices()\n",
    "        return combine_points_and_prices(self.query_points, prices)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff6d44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "price_map = PriceMap(reference_points_df.to_numpy(), floor=1)\n",
    "prices = price_map.compute_prices_df()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1c2343",
   "metadata": {},
   "source": [
    "We can visualise what we received:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951c5e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_map(prices: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Draw a map of prices using Plotly.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    prices: pd.DataFrame\n",
    "        DataFrame with columns x, y, floor, price\n",
    "    \"\"\"\n",
    "    fig = px.scatter(\n",
    "        prices,\n",
    "        x=\"x\",\n",
    "        y=\"y\",\n",
    "        color=\"price\",\n",
    "        color_continuous_scale=px.colors.sequential.Viridis,\n",
    "        title=\"Price Map (floor: {})\".format(prices[\"floor\"].iloc[0]),\n",
    "        # labels={\"x\": \"x\", \"y\": \"y\", \"floor\": \"floor\", \"price\": \"price\"},\n",
    "        width=600,\n",
    "        height=600,\n",
    "    )\n",
    "    fig.show()\n",
    "\n",
    "draw_map(prices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac39eca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "price_map = PriceMap(reference_points_df.to_numpy(), floor=1, points_per_dim=101, limit=5)\n",
    "prices = price_map.compute_prices_df()\n",
    "draw_map(prices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a83a20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "class PriceMapNaiveActor(PriceMap):\n",
    "    # We just keep it as is\n",
    "    pass\n",
    "\n",
    "\n",
    "price_map_actor = PriceMapNaiveActor.remote(reference_points_df.to_numpy(), points_per_dim=10, floor=1)\n",
    "task_id = price_map_actor.compute_prices_df.remote()\n",
    "ray.get(task_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9309eb",
   "metadata": {},
   "source": [
    "Our actor is currently very naive - it just runs everything as is.\n",
    "\n",
    "The following..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3607f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "@ray.remote\n",
    "class PriceMapActor(PriceMap):\n",
    "    def compute_prices(self) -> np.ndarray:\n",
    "        task_id = compute_prices_ray.remote(self.query_points, self.reference_points, k=DEFAULT_K)\n",
    "        return ray.get(task_id)\n",
    "\n",
    "\n",
    "price_map_actor = PriceMapActor.remote(reference_points_df.to_numpy(), points_per_dim=101, floor=1)\n",
    "task_id = price_map_actor.compute_prices_df.remote()\n",
    "ray.get(task_id)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56fd8df7",
   "metadata": {},
   "source": [
    "...is also naive.\n",
    "\n",
    "But we can implement the batching logic within the actor!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b821859f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "@ray.remote\n",
    "class PriceMapParallelActor(PriceMap):\n",
    "    BATCH_SIZE = 1000\n",
    "\n",
    "    _prices: Optional[np.ndarray] = None\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        reference_points: np.ndarray,\n",
    "        points_per_dim: int = PriceMap.DEFAULT_POINTS_PER_DIM,\n",
    "        floor: int = PriceMap.DEFAULT_FLOOR,\n",
    "        limit: float = PriceMap.DEFAULT_LIMIT,\n",
    "    ):\n",
    "        self.points_per_dim = points_per_dim\n",
    "        self.floor = floor\n",
    "        self.limit = limit\n",
    "        self.reference_points_id = reference_points\n",
    "        self._create_query_points(floor=floor)\n",
    "\n",
    "    def compute_prices(self) -> np.ndarray:\n",
    "        if self._prices is None:\n",
    "            task_ids = [\n",
    "                compute_prices_ray.remote(batch, self.reference_points_id, k=DEFAULT_K)\n",
    "                for batch in split_into_batches(self.query_points, self.BATCH_SIZE)\n",
    "            ]\n",
    "            self._prices = np.hstack(ray.get(task_ids))\n",
    "\n",
    "        return self._prices\n",
    "\n",
    "\n",
    "price_map_actor = PriceMapParallelActor.remote(reference_points_df.to_numpy(), points_per_dim=101, floor=1)\n",
    "task_id = price_map_actor.compute_prices_df.remote()\n",
    "prices = ray.get(task_id)\n",
    "prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0c8eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_id = price_map_actor.compute_prices_df.remote()\n",
    "prices = ray.get(task_id)\n",
    "prices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a574bea",
   "metadata": {},
   "source": [
    "**Advanced task**: Create a map actor that lazily computes the map for floors the user is interested in.\n",
    "\n",
    "1. In the initialisation, do not create query points or set `floor`, but create a nested PriceMapParallelActor for each floor you're interested in.\n",
    "2. Call `compute_prices_df` of a correct actor to get its output. The first call will be slow but the result is cached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34581b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "class MultiFloorParallelPriceMapActor:\n",
    "    \"\"\"Caching actor for multiple floors.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        reference_points: np.ndarray,\n",
    "        points_per_dim: int = PriceMap.DEFAULT_POINTS_PER_DIM,\n",
    "        limit: float = PriceMap.DEFAULT_LIMIT,\n",
    "    ):        \n",
    "        self.points_per_dim = points_per_dim\n",
    "        self.limit = limit\n",
    "        self.reference_points_id = reference_points\n",
    "        self._create_floor_actors()\n",
    "\n",
    "    def _create_floor_actors(self):\n",
    "        \"\"\"\n",
    "        Create actors for each floor.\n",
    "        \"\"\"\n",
    "        self.floor_actors = {\n",
    "            floor: PriceMapParallelActor.remote(self.reference_points_id, points_per_dim=self.points_per_dim, floor=floor, limit=self.limit)\n",
    "            for floor in range(1, 21)  # Assuming floors 1 to 20\n",
    "        }\n",
    "\n",
    "    def get_floor_prices_df(self, floor: int) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Get prices for a specific floor.\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        floor: int\n",
    "            The floor number to get prices for.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        prices: pd.DataFrame\n",
    "            DataFrame with columns x, y, floor, price for the specified floor.\n",
    "        \"\"\"\n",
    "        if floor not in self.floor_actors:\n",
    "            raise ValueError(f\"Floor {floor} not found.\")\n",
    "        \n",
    "        task_id = self.floor_actors[floor].compute_prices_df.remote()\n",
    "        return ray.get(task_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763296c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_actor = MultiFloorParallelPriceMapActor.remote(reference_points_df.to_numpy(), points_per_dim=101)\n",
    "task_id = map_actor.get_floor_prices_df.remote(1)\n",
    "prices = ray.get(task_id)\n",
    "prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b850e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see how we do with different floors\n",
    "for floor in [1, 5, 17]:\n",
    "    task_id = map_actor.get_floor_prices_df.remote(floor)\n",
    "    prices = ray.get(task_id)\n",
    "    draw_map(prices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f2d185",
   "metadata": {},
   "source": [
    "## Optional: ray + jax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5567d9",
   "metadata": {},
   "source": [
    "\n",
    "Ray itself does not do any optimisations, it just runs the code we gave it (a numpy one), in a parallel fashion. If we want to combine parallelisation and optimisation (a very good idea), we need to use some library to optimise the building bricks themselves.\n",
    "\n",
    "If only we had a good GPU... Do we, in Colab?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99731a4",
   "metadata": {},
   "source": [
    "First, reuse the jax-optimised version of functions from the previous notebooks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0325d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "def compute_distances_jax(query_points: jnp.ndarray, reference_points: jnp.ndarray, *, n_dim: int = 3) -> jnp.ndarray:\n",
    "    query_points = query_points[:, jnp.newaxis,:n_dim]\n",
    "    reference_points = reference_points[jnp.newaxis, :, :n_dim]\n",
    "    return jnp.sqrt(jnp.sum((reference_points - query_points) ** 2, axis=-1))\n",
    "\n",
    "compute_distances_jax = jax.jit(compute_distances_jax, static_argnames=[\"n_dim\"])\n",
    "\n",
    "def knn_search_jax(\n",
    "    query_points: jnp.ndarray,\n",
    "    reference_points: jnp.ndarray,\n",
    "    k: int,\n",
    "):\n",
    "    distances = compute_distances_jax(query_points, reference_points).T\n",
    "    _, nearest_indices = jax.lax.top_k(-distances.T, k)\n",
    "    return nearest_indices\n",
    "\n",
    "knn_search_jax = jax.jit(knn_search_jax, static_argnames=[\"k\"])\n",
    "\n",
    "def split_batches_jax(array: jnp.ndarray, max_size: int) -> list[jnp.ndarray]:\n",
    "    # Same!\n",
    "    return [array[i:i + max_size] for i in range(0, len(array), max_size)]\n",
    "\n",
    "split_batches_jax = jax.jit(split_batches_jax, static_argnames = [\"max_size\"])\n",
    "\n",
    "@ray.remote\n",
    "def compute_prices_jax_ray(query_points: jnp.ndarray, reference_points: jnp.ndarray, k: int = DEFAULT_K) -> jnp.ndarray:\n",
    "    indices = knn_search_jax(query_points, reference_points, k)\n",
    "    prices: jnp.ndarray = reference_points[indices, 3]\n",
    "    return prices.mean(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bceef7dd",
   "metadata": {},
   "source": [
    "And now we just replace the counterparts..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ddecb5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "@ray.remote\n",
    "class PriceMapParallelJaxActor(PriceMap):\n",
    "    BATCH_SIZE = 1000\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        reference_points: np.ndarray,\n",
    "        points_per_dim: int = PriceMap.DEFAULT_POINTS_PER_DIM,\n",
    "        floor: int = PriceMap.DEFAULT_FLOOR,\n",
    "        limit: float = PriceMap.DEFAULT_LIMIT,\n",
    "    ):\n",
    "        self.points_per_dim = points_per_dim\n",
    "        self.floor = floor\n",
    "        self.limit = limit\n",
    "        self.reference_points_id = ray.put(jnp.array(reference_points))\n",
    "        self._create_query_points()\n",
    "\n",
    "    def _create_query_points(self) -> None:\n",
    "        super()._create_query_points(floor=self.floor)\n",
    "        self.query_points = jnp.array(self.query_points)    \n",
    "\n",
    "    def compute_prices(self) -> np.ndarray:\n",
    "        task_ids = [\n",
    "            compute_prices_jax_ray.remote(batch, self.reference_points_id, k=DEFAULT_K)\n",
    "            for batch in split_batches_jax(self.query_points, self.BATCH_SIZE)\n",
    "        ]\n",
    "        results = ray.get(task_ids)\n",
    "        result = np.array(jnp.hstack(results), copy=False)\n",
    "        return result\n",
    "\n",
    "\n",
    "price_map_actor = PriceMapParallelJaxActor.remote(reference_points_df.to_numpy(), points_per_dim=101, floor=1)\n",
    "task_id = price_map_actor.compute_prices_df.remote()\n",
    "prices = ray.get(task_id)\n",
    "prices"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "accelerating-scientific-python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
