{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3497647e",
   "metadata": {},
   "source": [
    "# Parallel computation with Ray"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54735612",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/coobas/europython-25/blob/main/98-ray.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0bf1e03",
   "metadata": {},
   "source": [
    "[Ray](https://docs.ray.io/en/latest/index.html) is a set of libraries that (among others) allow an easy parallelisation of Python tasks - both locally and also in clusters. This part is called **Ray Core**.\n",
    "\n",
    "Apart from this, it also provides specialised libraries for data processing (**Ray Data**), for machine learning and even reinforcement learning (**Ray Train**, **Ray Train**, ...). We will not deal with those in this workshop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8631bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this in Google Collab, not needed if you install this package locally\n",
    "!pip install numpy ray[default]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8faadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obligatory imports\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import ray\n",
    "\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e000c854",
   "metadata": {},
   "outputs": [],
   "source": [
    "def long_running() -> int:\n",
    "    \"\"\"A long running task that we will parallelise.\"\"\"\n",
    "    import time\n",
    "    time.sleep(1)\n",
    "    return 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2c7a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "long_running()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc67e736",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "[long_running() for i in range(10)]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49eab1e0",
   "metadata": {},
   "source": [
    "If these tasks are independent, we can run them in parallel. There are of course options in Python itself:\n",
    "\n",
    "- multiprocessing (https://docs.python.org/3.13/library/multiprocessing.html) \n",
    "- threading (with GIL-releasing code or with caution in free-threaded Python 3.13+)\n",
    "\n",
    "This does scale exactly well if there are more tasks than CPUs / GPUs on your machine...\n",
    "\n",
    "Other options:\n",
    "- [celery](https://docs.celeryq.dev/en/stable/)\n",
    "- [dask](https://docs.dask.org/en/stable/index.html)\n",
    "\n",
    "With their strengths and weaknesses. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a5e9c9",
   "metadata": {},
   "source": [
    "## Use ray"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6e34b8",
   "metadata": {},
   "source": [
    "Ray always runs a server (even implicitly) and executes the task in nodes that it manages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11fc3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a96d3e3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55769580",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def long_running_ray() -> int:\n",
    "    import time\n",
    "    time.sleep(1)\n",
    "    return 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8dff515",
   "metadata": {},
   "outputs": [],
   "source": [
    "long_running_ray()  # This will fail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2e9046",
   "metadata": {},
   "outputs": [],
   "source": [
    "long_running_ray.remote()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f784399",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_id = long_running_ray.remote()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecad1b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.get(task_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d22b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "task_ids = [long_running_ray.remote() for i in range(10)]\n",
    "ray.get(task_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a324db",
   "metadata": {},
   "source": [
    "## Exercise: Compute prices at many points "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b321460",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08c34ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "N_POINTS = 10   # Number of points in each dimension for the grid\n",
    "LIMIT = 10.0    # +/- Span of the grid\n",
    "DEFAULT_K = 4   # How many nearest neighbors to consider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378c24e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_distances(query_points: np.ndarray, reference_points: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Calculate mutual distances between M query and N reference points.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    distances: np.ndarray\n",
    "        (N, M) array of the distances\n",
    "    \"\"\"\n",
    "    # Expand for broadcasting\n",
    "    query_points = query_points[:, :, np.newaxis]\n",
    "    reference_points = reference_points[:3, np.newaxis]\n",
    "    return np.sqrt(np.sum((reference_points - query_points) ** 2, axis=0))\n",
    "\n",
    "\n",
    "def knn_search(\n",
    "    query_points: np.ndarray,\n",
    "    reference_points: np.ndarray,\n",
    "    k: int,\n",
    "    distances_func=calculate_distances,\n",
    "):\n",
    "    \"\"\"\n",
    "    Find k nearest neighbour reference point indices for N query points.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    indices: np.ndarray\n",
    "        (N, k) matrix of integral indices\n",
    "\n",
    "    \"\"\"\n",
    "    distances = distances_func(query_points, reference_points).T\n",
    "    nearest_indices = np.argpartition(distances, k, axis=0)[:k].T\n",
    "    return nearest_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca859ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_point_grid(n_points: int = N_POINTS) -> tuple[np.ndarray, ...]:\n",
    "    \"\"\"\n",
    "    Create a homogenous grid of points to create a map.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    x: np.ndarray\n",
    "        Flattened (N_POINTS x N_POINTS,) array of x values\n",
    "    y: np.ndarray\n",
    "        Flattened (N_POINTS x N_POINTS,) array of x values\n",
    "    \"\"\"\n",
    "    # TODO: Add floor\n",
    "    x = np.linspace(-LIMIT, LIMIT, n_points)\n",
    "    y = np.linspace(-LIMIT, LIMIT, n_points)\n",
    "    return tuple(arr.flatten() for arr in np.meshgrid(x, y))\n",
    "\n",
    "\n",
    "def create_query_points(n_points: int = N_POINTS, floor: int = 1) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Create a homogenous grid of points with a floor to create a map.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    query_points: np.ndarray\n",
    "        (n_points x n_points, 3) array of query points\n",
    "    \"\"\"\n",
    "    x, y = create_point_grid(n_points=n_points)\n",
    "    return np.vstack([x, y, np.ones(x.shape[0]) * floor])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b91cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_prices(query_points: np.ndarray, reference_points: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Find prices for N data_points.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    prices: np.ndarray\n",
    "        (N,) array of prices\n",
    "    \"\"\"\n",
    "    indices = knn_search(query_points, reference_points, DEFAULT_K)\n",
    "    prices: np.ndarray = reference_points[3][indices]\n",
    "    return prices.mean(axis=1)\n",
    "\n",
    "\n",
    "def combine_points_and_prices(\n",
    "    query_points: np.ndarray, prices: np.ndarray\n",
    ") -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Prepare human-friendly output from numpy arrays.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    df: pl.DataFrame\n",
    "        DataFrame with columns x, y, floor, price\n",
    "    \"\"\"\n",
    "    return pl.DataFrame(\n",
    "        {\n",
    "            \"x\": query_points[0],\n",
    "            \"y\": query_points[1],\n",
    "            \"floor\": query_points[2],\n",
    "            \"price\": prices,\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a7774f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_reference_points(path: Path = Path(\"local_data/data.parquet\")) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Load reference data points from a Parquet file.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    data_points: np.ndarray\n",
    "        (N, 4) array of data points with x, y, floor, and price columns\n",
    "    \"\"\"\n",
    "\n",
    "    df = pl.read_parquet(path)\n",
    "    return np.vstack(\n",
    "        [\n",
    "            df[\"x\"].to_numpy(),\n",
    "            df[\"y\"].to_numpy(),\n",
    "            df[\"floor\"].to_numpy(),\n",
    "            df[\"price\"].to_numpy(),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "load_reference_points()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20bc510c",
   "metadata": {},
   "source": [
    "## Run without ray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c811011",
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_points = load_reference_points()\n",
    "query_points = create_query_points(n_points=21)  # 21x21 grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddcecfae",
   "metadata": {},
   "outputs": [],
   "source": [
    "prices = compute_prices(query_points, reference_points)\n",
    "prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8720f27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "points_and_prices = combine_points_and_prices(query_points=query_points, prices=prices)\n",
    "points_and_prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a8bc40",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def long_running_task():\n",
    "    import time\n",
    "    time.sleep(60)\n",
    "    return \"Finished long running task\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080caa3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.get(long_running_task.remote())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ddd048",
   "metadata": {},
   "source": [
    "## Monitoring ray"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a33f860",
   "metadata": {},
   "source": [
    "Ray comes with a nice dashboard that allows you to observe running jobs. It runs in a local web server, mostly likely http://localhost:8265. This address is not accessible when running within Google Colab, and so you have to use a special trick to show a mini-window forwarded to the dashboard running in the cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdfb2484",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from google.colab import output\n",
    "    output.serve_kernel_port_as_iframe(8265)  # The port may differ!\n",
    "except ImportError:\n",
    "    print(\"Not in google Colab. Try the local link, it might work.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ff5948",
   "metadata": {},
   "source": [
    "## Compute prices in ray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cfeece6",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def compute_prices(query_points: np.ndarray, reference_points: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Find prices for N data_points.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    prices: np.ndarray\n",
    "        (N,) array of prices\n",
    "    \"\"\"\n",
    "    indices = knn_search(query_points, reference_points, DEFAULT_K)\n",
    "    prices: np.ndarray = reference_points[3][indices]\n",
    "    return prices.mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ad7271",
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_points = load_reference_points()\n",
    "query_points = create_query_points(n_points=21)  \n",
    "\n",
    "ray.init(ignore_reinit_error=True)\n",
    "\n",
    "prices = compute_prices.remote(query_points, reference_points)\n",
    "prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790e553d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.get(prices)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "accelerating-scientific-python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
