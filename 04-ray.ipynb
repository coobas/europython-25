{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3497647e",
   "metadata": {},
   "source": [
    "# Parallel computation with Ray"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54735612",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/coobas/europython-25/blob/main/04-ray.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0bf1e03",
   "metadata": {},
   "source": [
    "[Ray](https://docs.ray.io/en/latest/index.html) is a set of libraries that (among others) allow an easy parallelisation of Python tasks - both locally and also in clusters. This part is called **Ray Core**.\n",
    "\n",
    "Apart from this, it also provides specialised libraries for data processing (**Ray Data**), for machine learning and even reinforcement learning (**Ray Train**, **Ray Train**, ...). We will not deal with those in this workshop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8631bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this in Google Collab, not needed if you install this package locally\n",
    "!pip install numpy ray[default]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8faadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obligatory imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ray\n",
    "import plotly.express as px\n",
    "from IPython import get_ipython\n",
    "\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce591e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "GOOGLE_COLAB = \"google.colab\" in str(get_ipython())\n",
    "\n",
    "# Download the data which are part of this repo\n",
    "if GOOGLE_COLAB:\n",
    "    import urllib\n",
    "    url = \"https://github.com/coobas/europython-25/raw/refs/heads/main/data.parquet\"\n",
    "    urllib.request.urlretrieve(url, \"data.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e000c854",
   "metadata": {},
   "outputs": [],
   "source": [
    "def long_running() -> int:\n",
    "    \"\"\"A long running task that we will parallelise.\"\"\"\n",
    "    import time\n",
    "    time.sleep(1)\n",
    "    return 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2c7a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "long_running()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc67e736",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "[long_running() for i in range(10)]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49eab1e0",
   "metadata": {},
   "source": [
    "If these tasks are independent, we can run them in parallel. There are of course options in Python itself:\n",
    "\n",
    "- multiprocessing (https://docs.python.org/3.13/library/multiprocessing.html) \n",
    "- threading (with GIL-releasing code or with caution in free-threaded Python 3.13+)\n",
    "\n",
    "This does scale exactly well if there are more tasks than CPUs / GPUs on your machine...\n",
    "\n",
    "Other options:\n",
    "- [celery](https://docs.celeryq.dev/en/stable/)\n",
    "- [dask](https://docs.dask.org/en/stable/index.html)\n",
    "\n",
    "With their strengths and weaknesses. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a5e9c9",
   "metadata": {},
   "source": [
    "## Use ray"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6e34b8",
   "metadata": {},
   "source": [
    "Ray always runs a server (even implicitly) and executes the task in nodes that it manages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11fc3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55769580",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def long_running_ray() -> int:\n",
    "    import time\n",
    "    time.sleep(1)\n",
    "    return 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8dff515",
   "metadata": {},
   "outputs": [],
   "source": [
    "long_running_ray()  # This will fail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2e9046",
   "metadata": {},
   "outputs": [],
   "source": [
    "long_running_ray.remote()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f784399",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_id = long_running_ray.remote()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecad1b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.get(task_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d22b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "task_ids = [long_running_ray.remote() for i in range(10)]\n",
    "ray.get(task_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a324db",
   "metadata": {},
   "source": [
    "## Exercise: Compute prices at many points "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f62167",
   "metadata": {},
   "source": [
    "We will reuse our definitions of kNN functions (slightly modified) and for random points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b321460",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "N_POINTS = 10   # Default number of points in each dimension for the grid\n",
    "LIMIT = 10.0    # +/- Span of the grid\n",
    "DEFAULT_K = 4   # How many nearest neighbors to consider\n",
    "\n",
    "\n",
    "def calculate_distances(query_points: np.ndarray, reference_points: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Calculate mutual Euclidean distances between M query and N reference points.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    query_points: np.ndarray\n",
    "        (M, 3) array of query points\n",
    "    reference_points: np.ndarray\n",
    "        (N, 3+) array of reference points\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    distances: np.ndarray\n",
    "        (M, N) array of the distances\n",
    "    \"\"\"\n",
    "    # Expand for broadcasting\n",
    "    query_points = query_points[:, np.newaxis,:3]\n",
    "    reference_points = reference_points[np.newaxis, :, :3]\n",
    "    return np.sqrt(np.sum((reference_points - query_points) ** 2, axis=-1))\n",
    "\n",
    "\n",
    "def knn_search(\n",
    "    query_points: np.ndarray,\n",
    "    reference_points: np.ndarray,\n",
    "    k: int,\n",
    "):\n",
    "    \"\"\"\n",
    "    Find k nearest neighbour reference point indices for N query points.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    indices: np.ndarray\n",
    "        (N, k) matrix of integral indices\n",
    "    \"\"\"\n",
    "    distances = calculate_distances(query_points, reference_points).T\n",
    "    return np.argpartition(distances, k, axis=0)[:k].T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b91cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_prices(query_points, reference_points, k: int = DEFAULT_K):\n",
    "    \"\"\"\n",
    "    Find prices for N data_points.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    query_points: np.ndarray\n",
    "        (N, 3) array of query points\n",
    "    reference_points: np.ndarray\n",
    "        (M, 4) array of data points with x, y, floor, and price\n",
    "    k: int\n",
    "        Number of nearest neighbors to consider\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    prices: np.ndarray\n",
    "        (N,) array of prices\n",
    "    \"\"\"\n",
    "    indices = knn_search(query_points, reference_points, k)\n",
    "    prices: np.ndarray = reference_points[indices, 3]\n",
    "    return prices.mean(axis=1)\n",
    "\n",
    "\n",
    "def combine_points_and_prices(\n",
    "    query_points: np.ndarray, prices: np.ndarray\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Prepare human-friendly output from numpy arrays.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    df: pd.DataFrame\n",
    "        DataFrame with columns x, y, floor, price\n",
    "    \"\"\"\n",
    "    return pd.DataFrame(\n",
    "        {\n",
    "            \"x\": query_points[:,0],\n",
    "            \"y\": query_points[:,1],\n",
    "            \"floor\": query_points[:,2],\n",
    "            \"price\": prices,\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767f7100",
   "metadata": {},
   "source": [
    "We have predefined reference points in an external file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f1f89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_reference_points_df(path: Path = Path(\"data.parquet\")) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Load reference data points from a parquet file.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    data_points: np.ndarray\n",
    "        (N, 4) array of data points with x, y, floor, and price columns\n",
    "    \"\"\"\n",
    "\n",
    "    return pd.read_parquet(path)\n",
    "    # return df[[\"x\", \"y\", \"floor\", \"price\"]].to_numpy().astype(float)\n",
    "\n",
    "reference_points_df = load_reference_points_df()\n",
    "reference_points_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c1769b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_random_points(\n",
    "    n_points: int, n_dim: int = 3, *, seed: int = 42\n",
    ") -> np.ndarray:\n",
    "    # TODO: Fix floor!\n",
    "    np.random.seed(seed)\n",
    "    return np.random.sample((n_points, n_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b33ba5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_points = create_random_points(10)\n",
    "query_points    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b5b7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "prices = compute_prices(query_points, reference_points)\n",
    "prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08da8537",
   "metadata": {},
   "outputs": [],
   "source": [
    "combine_points_and_prices(query_points, prices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67901582",
   "metadata": {},
   "source": [
    "**Tasks**\n",
    "1) Create a remote function variant of compute_prices (called e.g. compute_prices_ray)\n",
    "2) Run it in ray and get the result\n",
    "3) Compare the result to the previous\n",
    "4) Compare the execution time (for a larger number of points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8513905a",
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "395fe9e5",
   "metadata": {},
   "source": [
    "**Question**\n",
    "\n",
    "Did we achieve anything sofar?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0070ca58",
   "metadata": {},
   "source": [
    "## Monitoring ray"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc9e6d7",
   "metadata": {},
   "source": [
    "Ray comes with a nice dashboard that allows you to observe running jobs. It runs in a local web server, mostly likely http://localhost:8265. This address is not accessible when running within Google Colab, and so you have to use a special trick to show a mini-window forwarded to the dashboard running in the cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2bf7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if GOOGLE_COLAB:\n",
    "    from google.colab import output\n",
    "    output.serve_kernel_port_as_iframe(8265)  # The port may differ!\n",
    "else:\n",
    "    print(\"Not in google Colab. Try the local link, it should work.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d9398c",
   "metadata": {},
   "source": [
    "TODO: Submit objects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab25262",
   "metadata": {},
   "source": [
    "TODO: Batching"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8a7388",
   "metadata": {},
   "source": [
    "## Exercise: Create a map of prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca859ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_grid(n_points: int = N_POINTS) -> tuple[np.ndarray, ...]:\n",
    "    \"\"\"\n",
    "    Create a homogenous grid of points to create a map.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    x: np.ndarray\n",
    "        Flattened (n_points x n_points,) array of x values\n",
    "    y: np.ndarray\n",
    "        Flattened (n_points x n_points,) array of x values\n",
    "    \"\"\"\n",
    "    # Note: Tested indirectly via `create_query_points`\n",
    "    # TODO: Add floor\n",
    "    x = np.linspace(-LIMIT, LIMIT, n_points)\n",
    "    y = np.linspace(-LIMIT, LIMIT, n_points)\n",
    "    return tuple(arr.flatten() for arr in np.meshgrid(x, y))\n",
    "\n",
    "\n",
    "def create_query_points(n_points: int = N_POINTS, floor: int = 1) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Create a homogenous grid of points with a floor to create a map.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    query_points: np.ndarray\n",
    "        (n_points x n_points, 3) array of query points\n",
    "    \"\"\"\n",
    "    x, y = create_grid(n_points=n_points)\n",
    "    return np.vstack([x, y, np.ones(x.shape[0]) * floor]).T\n",
    "\n",
    "\n",
    "create_query_points(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3607f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_points(points: np.ndarray) -> None:\n",
    "    \"\"\"\n",
    "    Draw points on a map.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    points: np.ndarray\n",
    "        (N, 3) array of points to draw\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame({\"x\": points[:,0], \"y\": points[:,1]})\n",
    "    fig = px.scatter(df, x=\"x\", y=\"y\", title=\"Query points\")\n",
    "    fig.show()\n",
    "\n",
    "draw_points(create_query_points(5))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "accelerating-scientific-python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
